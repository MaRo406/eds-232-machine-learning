{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Discussion 4\n",
        "subtitle: Prediciting National Park Species using Decision Trees & KNN\n",
        "description: 'Thursday, January 30th, 2025'\n",
        "jupyter:\n",
        "  jupytext:\n",
        "    text_representation:\n",
        "      extension: .qmd\n",
        "      format_name: quarto\n",
        "      format_version: '1.0'\n",
        "      jupytext_version: 1.16.4\n",
        "  kernelspec:\n",
        "    display_name: EDS 232 Environment\n",
        "    language: python\n",
        "    name: eds232_env\n",
        "---\n",
        "\n",
        "\n",
        "### Introduction\n",
        "\n",
        "In this week's discussion section, we will run and tune a KNN and Decision Tree model. We will see how our models perform differently, as well as attempt to improve them!  Given the variables listed below, we are going to build our model to predict which National Park a given taxon is part of. \n",
        "\n",
        "### Data \n",
        "\n",
        "Our data for this discussion section comes from a [tidytuesday project](https://github.com/rfordatascience/tidytuesday). You can access the data [on this google sheet](https://docs.google.com/spreadsheets/d/1m2NfaV6HDf0jetd35SwskcPcYiELja7WQrpnrcsucsQ/edit?gid=715770001#gid=715770001) and download as a CSV. The dataset contains National Park species obserations for the 15 most popular National Parks in the U.S. The [user manual](https://irma.nps.gov/content/npspecies/Help/docs/NPSpecies_User_Guide.pdf) provided by the NPS cotains information on the different variables. Check out the table below with the different variables we will use this week. \n",
        "\n",
        "| Feature                     | Description                                                                                   |\n",
        "|-----------------------------|-----------------------------------------------------------------------------------------------|\n",
        "| CategoryName        |   Broad taxanomic grouping                       |\n",
        "| Order                |       Order of Species                            |\n",
        "| Family                   |       Taxanomic family                                             |\n",
        "| GRank                   |     Global Rank, An assesment of the level of rarity or abundance of a taxon                                                  |\n",
        "| ParkName              |          National Park Name                                                         |\n",
        "| Sensitive                |   True/False variable on whether or not the taxon is sensitive                            |\n",
        "| Nativeness   | True/False variable on whether or not the taxon is native to the park   |\n",
        "| Abundance           | Categorical variable specifying whether the taxons are rare, common, or abundant|\n",
        "| Observations           | The number of times the taxon has been observed |\n",
        "\n",
        "## Time to dive in to our models!\n",
        "\n",
        "### 1. Prepare the data\n",
        "\n",
        "Read in the national park data and prepare your data from your models. We want our target variable to be `ParkName`, and all other variables in the table above to be our features. Then, split and scale the data. \n"
      ],
      "id": "1774a429"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import  accuracy_score\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "id": "f7ef9631",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Read in national park data\n",
        "np_df = pd.read_csv('../data/national_park_species.csv')\n",
        "np_df = np_df.drop(np_df.columns[0], axis=1)\n",
        "pd.set_option('display.max_columns', None)\n",
        "np_df.head()"
      ],
      "id": "8416b96a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Encode categorical variables\n",
        "for col in ['CategoryName', 'Order', 'Family', 'GRank', 'ParkName', 'Sensitive', 'Nativeness', 'Abundance','Observations', 'GRank']:\n",
        "    np_df[f\"{col}_cat\"] = np_df[col].astype('category').cat.codes\n",
        "\n",
        "# Split data into X and y\n",
        "X = np_df[['CategoryName_cat', 'Order_cat', 'Family_cat', 'GRank_cat', 'Sensitive_cat', 'Nativeness_cat', 'Abundance_cat','Observations_cat', 'GRank_cat']]\n",
        "y= np_df['ParkName_cat']\n",
        "\n",
        "# Split data into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.33, random_state = 42)\n",
        "\n",
        "# Standardize the predictors\n",
        "scaler = StandardScaler() \n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "id": "a4630dc2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Create a KNN Classifier \n",
        "After running an untuned model, iterate over different values of K to see which performs best. Then, visualize how your accuracy changes wtih a varying K.\n"
      ],
      "id": "b4b88e83"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize KNN classiier\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "y_pred = knn.predict(X_test_scaled)"
      ],
      "id": "d3d933f3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(accuracy)"
      ],
      "id": "c316fb3c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "### Tune a KNN Model and Visualize results\n",
        "def knn():\n",
        "    # Different k values to iterate over\n",
        "    k_values = [3, 5, 7, 9, 11]\n",
        "    accuracies = []\n",
        "\n",
        "    # Create KNN model for each value of K, fit/predict, and calculate accuracies\n",
        "    for k in k_values:\n",
        "        knn = KNeighborsClassifier(n_neighbors=k)\n",
        "        knn.fit(X_train_scaled, y_train)\n",
        "        \n",
        "        # Make predictions and calculate accuracy\n",
        "        y_pred = knn.predict(X_test_scaled)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        accuracies.append(accuracy)\n",
        "        \n",
        "        print(f\"K={k} - Accuracy: {accuracy:.3f}\")\n",
        "    \n",
        "    # Visualize K against accuracy\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(k_values, accuracies, marker='o')\n",
        "    plt.xlabel('Number of Neighbors (K)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('KNN: Effect of K on Model Accuracy')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "knn()"
      ],
      "id": "91a1551d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Create a Decision Tree\n",
        "After running an untuned model, iterate over different max depths for your decision tree to determine which performs best. Then,create a decision tree visual using `plot_tree`. Lastly, find the most important features using `.feature_importances_`.\n"
      ],
      "id": "d6dae0aa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize Decision Tree classiier\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_train_scaled, y_train)\n",
        "y_pred = dt.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(accuracy)"
      ],
      "id": "e7706b0d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Max depths to iterate over\n",
        "max_depths = [2, 3, 4, 5]\n",
        "accuracies = []\n",
        "\n",
        "# Create decision tree model for different depths and report accuracies\n",
        "for depth in max_depths:\n",
        "    dt = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
        "    dt.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    # Make predictions and calculate accuracy\n",
        "    y_pred = dt.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "    \n",
        "    print(f\"Max Depth: {depth} - Accuracy: {accuracy:.3f}\")"
      ],
      "id": "d3abd415",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visualize Models\n"
      ],
      "id": "5af80fd4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create and fit model with best depth\n",
        "dt_best = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "dt_best.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Create the mapping from numeric classes to descriptive names\n",
        "class_mapping = dict(zip(dt_best.classes_, np_df.ParkName.unique()))\n",
        "\n",
        "# Convert class labels in dt.classes_ to strings using the mapping\n",
        "class_names_str = [class_mapping[cls] for cls in dt_best.classes_]\n",
        "\n",
        "# Plot decision tree\n",
        "plt.figure(figsize=(12, 15), dpi=700)\n",
        "plot_tree(dt_best, feature_names=X.columns, class_names= class_names_str, \n",
        "          filled=True, rounded=True)\n",
        "plt.title(\"Decision Tree Visualization (max_depth = 5)\")\n",
        "plt.savefig('decision_tree.png') \n",
        "plt.show()"
      ],
      "id": "80fa43d6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Find important features\n"
      ],
      "id": "3d278e1a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': dt_best.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importance:\")\n",
        "print(feature_importance)"
      ],
      "id": "1ee57da4",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "EDS 232 Environment",
      "language": "python",
      "name": "eds232_env"
    },
    "kernel": "eds232_env",
    "jupytext": {
      "text_representation": {
        "extension": ".qmd",
        "format_name": "quarto",
        "format_version": "1.0",
        "jupytext_version": "1.16.4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}