{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Discussion 9\n",
        "subtitle: Support Vector Machines with simulated data\n",
        "description: 'Thursday, March 6th, 2025'\n",
        "jupyter:\n",
        "  jupytext:\n",
        "    text_representation:\n",
        "      extension: .qmd\n",
        "      format_name: quarto\n",
        "      format_version: '1.0'\n",
        "      jupytext_version: 1.16.4\n",
        "  kernelspec:\n",
        "    display_name: EDS 232\n",
        "    language: python\n",
        "    name: eds232-env\n",
        "---\n",
        "\n",
        "\n",
        "### Introduction\n",
        "\n",
        "In this week's discussion section, we will use simulated datasets to create a widget that looks at the results of a Support Vector Machine. The simulated datasents represent different relationships within our data. Our widget will allow us to select different kernels, regularization parameters, and methods for calculating gamma, allowing us to see how these changes change the classification of our data. The image below serves as a reminder of a few of the different kernels we can use in a Support Vector Machine. \n",
        "\n",
        "![](svm_kernels.png)\n",
        "\n",
        "### Data \n",
        "\n",
        "As mentioned above, the data we will be working with this week is all simulated. Copy the code cell below to obtain the data. \n",
        "\n",
        "### Excercise\n",
        "\n",
        "#### Load in libraries\n"
      ],
      "id": "38ff1623"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "id": "4eb3efb4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create simulated data\n"
      ],
      "id": "60321727"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate Linearly Separable Data\n",
        "X1 = np.random.randn(100, 2) + [2, 2]\n",
        "X2 = np.random.randn(100, 2) + [5, 5]\n",
        "X_linear = np.vstack((X1, X2))\n",
        "y_linear = np.array([0]*100 + [1]*100)\n",
        "df_linear = pd.DataFrame(X_linear, columns=['Temperature', 'Humidity'])\n",
        "df_linear['Pollution_Level'] = y_linear\n",
        "\n",
        "# Generate Non-linearly Separable Data (Circular Boundaries)\n",
        "length = 200\n",
        "radius = 2\n",
        "angle = np.linspace(0, 2 * np.pi, length)\n",
        "X1_circular = np.vstack((np.sin(angle) * radius, np.cos(angle) * radius)).T + np.random.randn(length, 2) * 0.1\n",
        "X2_circular = np.random.randn(length, 2) * 0.5\n",
        "X_circular = np.vstack((X1_circular, X2_circular))\n",
        "y_circular = np.array([0] * length + [1] * length)\n",
        "df_circular = pd.DataFrame(X_circular, columns=['CO2_Emission', 'Water_Usage'])\n",
        "df_circular['Area_Type'] = y_circular\n",
        "\n",
        "# Generate XOR-like Data\n",
        "X1_xor = np.random.randn(50, 2) + [2, 2]\n",
        "X2_xor = np.random.randn(50, 2) + [2, 5]\n",
        "X3_xor = np.random.randn(50, 2) + [5, 2]\n",
        "X4_xor = np.random.randn(50, 2) + [5, 5]\n",
        "X_xor = np.vstack((X1_xor, X2_xor, X3_xor, X4_xor))\n",
        "y_xor = np.array([0]*100 + [1]*100)\n",
        "df_xor = pd.DataFrame(X_xor, columns=['Species_Count', 'Toxicity_Level'])\n",
        "df_xor['Habitat_Damage'] = y_xor\n",
        "\n",
        "# Generate Overlapping Data\n",
        "X1_overlap = np.random.randn(100, 2) + [3, 3]\n",
        "X2_overlap = np.random.randn(100, 2) + [4, 4]\n",
        "X_overlap = np.vstack((X1_overlap, X2_overlap))\n",
        "y_overlap = np.array([0]*100 + [1]*100)\n",
        "df_overlap = pd.DataFrame(X_overlap, columns=['Air_Quality', 'Noise_Level'])\n",
        "df_overlap['Health_Risk'] = y_overlap\n",
        "\n",
        "# Collect all datasets in a dictionary for easy access\n",
        "datasets = {\n",
        "    'linear_data': df_linear,\n",
        "    'circular_data': df_circular,\n",
        "    'xor_data': df_xor,\n",
        "    'overlapping_data': df_overlap\n",
        "}"
      ],
      "id": "f9261a85",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create a SVM Widget ( Note that widget does not work in browser, you must run it locally!) \n"
      ],
      "id": "3797bad9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def interactive_svm(kernel, C, gamma, data_key):\n",
        "    \"\"\"\n",
        "    Widget SVM  Function \n",
        "    \n",
        "    Parameters:\n",
        "    - kernel: SVM kernel type\n",
        "    - C: Regularization parameter\n",
        "    - gamma: Kernel coefficient (auto/ scale)\n",
        "    - data: DataFrame with features and target as the last column\n",
        "    \"\"\"\n",
        "    \n",
        "    # Separate features and target\n",
        "    data = datasets[data_key]\n",
        "    X = data.iloc[:, :-1]\n",
        "    y = data.iloc[:, -1]\n",
        "    feature_names = X.columns # used for plotting axis labels\n",
        "    target_name = data.columns[-1] # used for plotting legend \n",
        "    \n",
        "    # Preprocess data\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "    \n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "    \n",
        "    # Train SVM model\n",
        "    model = SVC(kernel=kernel, C=C, gamma=gamma)\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Evaluate the model\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Plot decision boundaries if dataset has 2 features\n",
        "    if X.shape[1] == 2:\n",
        "        fig, ax = plt.subplots(figsize=(10, 6)) \n",
        "        \n",
        "        # Create mesh for decision boundary\n",
        "\n",
        "         # Get min and max for x1 and x2, extend by one unit for each\n",
        "        x1_min, x1_max = X_test.iloc[:, 0].min() - 1, X_test.iloc[:, 0].max() + 1\n",
        "        x2_min, x2_max = X_test.iloc[:, 1].min() - 1, X_test.iloc[:, 1].max() + 1\n",
        "        \n",
        "        # np.meshgrid is a NumPy function that creates a rectangular grid from coordinate arrays\n",
        "        xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 100),\n",
        "                             np.linspace(x2_min, x2_max, 100))\n",
        "        \n",
        "        # Predict for mesh\n",
        "        \n",
        "        # ravel flattens a 2D array into a 1d array\n",
        "        # np.c stacks data side by side, so they are a coordinate\n",
        "        y_pred_input = np.c_[xx1.ravel(), xx2.ravel()] \n",
        "        y_pred_input_df = pd.DataFrame(y_pred_input, columns = feature_names[:2]) # make into dataframe to predict\n",
        "        y_pred = model.predict(y_pred_input_df) # Predict the class for each point in the meshgrid\n",
        "        y_pred = y_pred.reshape(xx1.shape) # Convert back into 2d array that matches shape of mesh grid in order to create contour plot\n",
        "        \n",
        "        \n",
        "        # Print accuracy\n",
        "        print(\"Accuracy Score\\n\")\n",
        "        print(accuracy_score(y_test, predictions))\n",
        "        \n",
        "        # Plot decision boundary using filled contour plot\n",
        "        plt.contourf(xx1, xx2, y_pred, alpha=0.4, cmap=plt.cm.RdYlBu)\n",
        "\n",
        "        # Plot original X scaled data as a scatter plot on top of contour plot\n",
        "        scatter = plt.scatter(X_scaled.iloc[:, 0], X_scaled.iloc[:, 1], \n",
        "                              c=y, cmap=plt.cm.RdYlBu) \n",
        "        \n",
        "        # Define the colors you used in your scatter plot\n",
        "        colors = [plt.cm.RdYlBu(i) for i in np.linspace(0, 1, len(np.unique(y)))]\n",
        "        \n",
        "        # Create a list of patches for the legend\n",
        "        # [0], [0] are trivial values used for where the line would be drawn in the Line2d function\n",
        "        legend_handles = [Line2D([0], [0], marker='o', color='w', label=f'{label}',\n",
        "               markerfacecolor=color, markersize=10, linestyle='none') for color, label in zip(colors, np.unique(y))]\n",
        "        \n",
        "        # Add the custom legend to the plot\n",
        "        ax.legend(handles=legend_handles, loc='upper right', title=f'{target_name}')\n",
        "\n",
        "        # Add legend labels and title\n",
        "        plt.xlabel(X.columns[0])\n",
        "        plt.ylabel(X.columns[1])\n",
        "        plt.title(f'SVM Decision Boundary (Kernel: {kernel})')\n",
        "        plt.show()\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# Create kernel widget\n",
        "kernel_widget = widgets.Dropdown(\n",
        "    options=['linear', 'rbf', 'poly'], \n",
        "    value='rbf', \n",
        "    description='Kernel:'\n",
        ")\n",
        "\n",
        "\n",
        "# Create regularization parameter widget\n",
        "C_widget = widgets.FloatLogSlider(\n",
        "    value=1.0, \n",
        "    base=10, \n",
        "    min=-3, \n",
        "    max=3, \n",
        "    description='C (Regularization):'\n",
        ")\n",
        "\n",
        "# Create gamma widget\n",
        "gamma_widget = widgets.Dropdown(\n",
        "    options=['scale', 'auto'], # scale -> gamma =  1/(n_features * X.var()) , auto -> gamma =  1 / n_features\n",
        "    value='scale', \n",
        "    description='Gamma:'\n",
        ")\n",
        "\n",
        "\n",
        "# Create dataset widget\n",
        "dataset_widget = widgets.Dropdown(\n",
        "    options=list(datasets.keys()),  # Dictionary keys as options\n",
        "    value='linear_data', \n",
        "    description='Dataset:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Piece together widget function and all input widget components \n",
        "widgets.interactive(\n",
        "    interactive_svm, \n",
        "    kernel=kernel_widget, \n",
        "    C=C_widget, \n",
        "    gamma=gamma_widget,\n",
        "    data_key=dataset_widget \n",
        ")"
      ],
      "id": "ac05f2ca",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "EDS 232",
      "language": "python",
      "name": "eds232-env"
    },
    "kernel": "eds232_env",
    "jupytext": {
      "text_representation": {
        "extension": ".qmd",
        "format_name": "quarto",
        "format_version": "1.0",
        "jupytext_version": "1.16.4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}