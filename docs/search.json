[
  {
    "objectID": "discussion/week4.html",
    "href": "discussion/week4.html",
    "title": "Discussion 4",
    "section": "",
    "text": "In this week’s discussion section, we will run and tune a KNN and Decision Tree model. We will see how our models perform differently, as well as attempt to improve them! Given the variables listed below, we are going to build our model to predict which National Park a given taxon is part of."
  },
  {
    "objectID": "discussion/week4.html#time-to-dive-in-to-our-models",
    "href": "discussion/week4.html#time-to-dive-in-to-our-models",
    "title": "Discussion 4",
    "section": "Time to dive in to our models!",
    "text": "Time to dive in to our models!\n\n1. Prepare the data\nRead in the national park data and prepare your data from your models. We want our target variable to be ParkName, and all other variables in the table above to be our features. Then, split and scale the data.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import  accuracy_score\nfrom sklearn.tree import plot_tree\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Read in national park data\nnp_df = pd.read_csv('../data/national_park_species.csv')\nnp_df = np_df.drop(np_df.columns[0], axis=1)\npd.set_option('display.max_columns', None)\nnp_df.head()\n\n\n\n\n\n\n\n\nParkCode\nParkName\nCategoryName\nOrder\nFamily\nTaxonRecordStatus\nSciName\nCommonNames\nParkAccepted\nSensitive\nRecordStatus\nOccurrence\nOccurrenceTags\nNativeness\nNativenessTags\nAbundance\nNPSTags\nParkTags\nReferences\nObservations\nVouchers\nExternalLinks\nTEStatus\nStateStatus\nOzoneSensitiveStatus\nGRank\nSRank\n\n\n\n\n0\nACAD\nAcadia National Park\nMammal\nArtiodactyla\nCervidae\nActive\nAlces alces\nMoose\nTrue\nFalse\nApproved\nPresent\nNaN\nNative\nNaN\nRare\nResident\nNaN\n11\n1\n0\nNaN\n50\nNaN\nNaN\nG5\nME: S5\n\n\n1\nACAD\nAcadia National Park\nMammal\nArtiodactyla\nCervidae\nActive\nOdocoileus virginianus\nNorthern White-tailed Deer, Virginia Deer, Whi...\nTrue\nFalse\nApproved\nPresent\nNaN\nNative\nNaN\nAbundant\nNaN\nNaN\n20\n0\n0\nNaN\n50\nNaN\nNaN\nG5\nME: S5\n\n\n2\nACAD\nAcadia National Park\nMammal\nCarnivora\nCanidae\nActive\nCanis latrans\nCoyote, Eastern Coyote\nTrue\nFalse\nApproved\nPresent\nNaN\nNon-native\nNaN\nCommon\nNaN\nNaN\n8\n1\n0\nNaN\nSC\nNaN\nNaN\nG5\nME: S5\n\n\n3\nACAD\nAcadia National Park\nMammal\nCarnivora\nCanidae\nActive\nCanis lupus\nEastern Timber Wolf, Gray Wolf, Timber Wolf\nTrue\nFalse\nApproved\nUnconfirmed\nNaN\nNative\nNaN\nNaN\nNaN\nNaN\n2\n0\n0\nNaN\nE\nNaN\nNaN\nG5\nME: SH\n\n\n4\nACAD\nAcadia National Park\nMammal\nCarnivora\nCanidae\nActive\nVulpes vulpes\nBlack Fox, Cross Fox, Eastern Red Fox, Fox, Re...\nTrue\nFalse\nApproved\nPresent\nNaN\nUnknown\nNaN\nCommon\nBreeder\nNaN\n16\n0\n0\nNaN\nNaN\nNaN\nNaN\nG5\nME: S5\n\n\n\n\n\n\n\n\n# Encode categorical variables\nfor col in ['CategoryName', 'Order', 'Family', 'GRank', 'ParkName', 'Sensitive', 'Nativeness', 'Abundance','Observations', 'GRank']:\n    np_df[f\"{col}_cat\"] = np_df[col].astype('category').cat.codes\n\n# Split data into X and y\nX = np_df[['CategoryName_cat', 'Order_cat', 'Family_cat', 'GRank_cat', 'Sensitive_cat', 'Nativeness_cat', 'Abundance_cat','Observations_cat', 'GRank_cat']]\ny= np_df['ParkName_cat']\n\n# Split data into training and testing\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.33, random_state = 42)\n\n# Standardize the predictors\nscaler = StandardScaler() \nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\n\n2. Create a KNN Classifier\nAfter running an untuned model, iterate over different values of K to see which performs best. Then, visualize how your accuracy changes wtih a varying K.\n\n# Initialize KNN classiier\nknn = KNeighborsClassifier()\nknn.fit(X_train_scaled, y_train)\ny_pred = knn.predict(X_test_scaled)\n\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(accuracy)\n\n0.6029747149231532\n\n\n\n### Tune a KNN Model and Visualize results\ndef knn():\n    # Different k values to iterate over\n    k_values = [3, 5, 7, 9, 11]\n    accuracies = []\n\n    # Create KNN model for each value of K, fit/predict, and calculate accuracies\n    for k in k_values:\n        knn = KNeighborsClassifier(n_neighbors=k)\n        knn.fit(X_train_scaled, y_train)\n        \n        # Make predictions and calculate accuracy\n        y_pred = knn.predict(X_test_scaled)\n        accuracy = accuracy_score(y_test, y_pred)\n        accuracies.append(accuracy)\n        \n        print(f\"K={k} - Accuracy: {accuracy:.3f}\")\n    \n    # Visualize K against accuracy\n    plt.figure(figsize=(10,6))\n    plt.plot(k_values, accuracies, marker='o')\n    plt.xlabel('Number of Neighbors (K)')\n    plt.ylabel('Accuracy')\n    plt.title('KNN: Effect of K on Model Accuracy')\n    plt.grid(True)\n    plt.show()\n\nknn()\n\nK=3 - Accuracy: 0.593\nK=5 - Accuracy: 0.603\nK=7 - Accuracy: 0.600\nK=9 - Accuracy: 0.600\nK=11 - Accuracy: 0.597\n\n\n\n\n\n\n\n3. Create a Decision Tree\nAfter running an untuned model, iterate over different max depths for your decision tree to determine which performs best. Then,create a decision tree visual using plot_tree. Lastly, find the most important features using .feature_importances_.\n\n# Initialize Decision Tree classiier\ndt = DecisionTreeClassifier()\ndt.fit(X_train_scaled, y_train)\ny_pred = dt.predict(X_test_scaled)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(accuracy)\n\n0.6353495290034705\n\n\n\n# Max depths to iterate over\nmax_depths = [2, 3, 4, 5]\naccuracies = []\n\n# Create decision tree model for different depths and report accuracies\nfor depth in max_depths:\n    dt = DecisionTreeClassifier(max_depth=depth, random_state=42)\n    dt.fit(X_train_scaled, y_train)\n    \n    # Make predictions and calculate accuracy\n    y_pred = dt.predict(X_test_scaled)\n    accuracy = accuracy_score(y_test, y_pred)\n    accuracies.append(accuracy)\n    \n    print(f\"Max Depth: {depth} - Accuracy: {accuracy:.3f}\")\n\nMax Depth: 2 - Accuracy: 0.466\nMax Depth: 3 - Accuracy: 0.497\nMax Depth: 4 - Accuracy: 0.532\nMax Depth: 5 - Accuracy: 0.547\n\n\n\nVisualize Models\n\n# Create and fit model with best depth\ndt_best = DecisionTreeClassifier(max_depth=5, random_state=42)\ndt_best.fit(X_train_scaled, y_train)\n\n# Create the mapping from numeric classes to descriptive names\nclass_mapping = dict(zip(dt_best.classes_, np_df.ParkName.unique()))\n\n# Convert class labels in dt.classes_ to strings using the mapping\nclass_names_str = [class_mapping[cls] for cls in dt_best.classes_]\n\n# Plot decision tree\nplt.figure(figsize=(12, 15), dpi=700)\nplot_tree(dt_best, feature_names=X.columns, class_names= class_names_str, \n          filled=True, rounded=True)\nplt.title(\"Decision Tree Visualization (max_depth = 5)\")\nplt.savefig('decision_tree.png') \nplt.show()\n\n\n\n\n\n\nFind important features\n\nfeature_importance = pd.DataFrame({\n    'feature': X.columns,\n    'importance': dt_best.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"\\nFeature Importance:\")\nprint(feature_importance)\n\n\nFeature Importance:\n            feature  importance\n5    Nativeness_cat    0.705087\n0  CategoryName_cat    0.126157\n7  Observations_cat    0.092148\n6     Abundance_cat    0.057482\n1         Order_cat    0.009900\n3         GRank_cat    0.008710\n8         GRank_cat    0.000359\n2        Family_cat    0.000159\n4     Sensitive_cat    0.000000"
  },
  {
    "objectID": "discussion/week3.html",
    "href": "discussion/week3.html",
    "title": "Discussion 3",
    "section": "",
    "text": "In this week’s discussion section, we will create some plots to better undertsand how much class imbalances can effect our classification model. Rather than creating a widget that updates the parameters of the model ( like we have done in the past couple weeks), this week we will create a widget that updates our data - specifically updating the class imbalance within our data. To do so, we will use synthesized data that is made with the intention of better understanding how relationships within data for logistic regression work. It is important to note that your results with real data may look very different - unlike this notebook, the real world data you will be working with was not made to better understand logistic regression."
  },
  {
    "objectID": "discussion/week3.html#data",
    "href": "discussion/week3.html#data",
    "title": "Discussion 3",
    "section": "Data",
    "text": "Data\nWhile our data is synthetic, we will still have it hold an environmnetal value. Our data is going to represent the prescence/absence of the invasive European green crab that is often found in California coastal waters. These crabs prefer warmer water temperatures between 64° F and 79° F and salinity levels between 26 and 39 ppt. The features for our data will be water temperature and salinity, and our target variable will be the presence (1) or absence (0) of green crabs at our different sampling sites. Import the libraries below and copy the function to generate our data below to get started.\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact, FloatSlider\nimport seaborn as sns"
  },
  {
    "objectID": "discussion/week3.html#time-for-some-functions",
    "href": "discussion/week3.html#time-for-some-functions",
    "title": "Discussion 3",
    "section": "Time for some FUN(ctions)!",
    "text": "Time for some FUN(ctions)!\nWe will create six different functions for the different parts of our interactive output: one to generate the data, a second to create a barplot to represent the class imbalance, a third to create a confusion matrix, another to create an ROC curve, a function to wrap everything together, and a final function to add interactivity. Let’s get to it!\n\nFunction 1\nCreate a function that generates the species data. The parameters should be the sample size and the ratio of present green crabs.\n\ndef generate_species_data(n_samples=1000, presence_ratio=0.3):\n    # Calculate number of samples for each class\n    n_present = int(n_samples * presence_ratio)\n    n_absent = n_samples - n_present\n    \n    # Generate features for presence sites \n    # Green crabs prefer warmer temps (between 64 and 79 degrees Fahrenheit) and  salinity between 26 and 39 ppt\n    temp_present = np.random.normal(loc=71, scale= 4, size=n_present)\n    salinity_present = np.random.normal(loc=32, scale=3, size=n_present)\n    X_present = np.column_stack([temp_present, salinity_present])\n    y_present = np.ones(n_present)\n    \n    # Generate features for absence sites\n    # Sites with warmer temps or lower salinity\n    temp_absent = np.random.normal(loc=26, scale=3, size=n_absent)\n    salinity_absent = np.random.normal(loc=28, scale=2, size=n_absent)\n    X_absent = np.column_stack([temp_absent, salinity_absent])\n    y_absent = np.zeros(n_absent)\n    \n    # Combine and shuffle the data\n    X = np.concatenate([X_present, X_absent])\n    y = np.concatenate([y_present, y_absent])\n    \n    # Shuffle the data\n    shuffle_idx = np.random.permutation(n_samples)\n    X = X[shuffle_idx]\n    y = y[shuffle_idx]\n    \n    return X, y\n\n\n\nFunction 2\nCreate a function that creats a bar plot of species presense distribution based on the ratio selected by the user.\n\ndef plot_class_distribution(y):\n    plt.figure(figsize = (8,4))\n    \n    # Count the values in each category\n    class_counts = pd.Series(y).value_counts()\n    \n    # Create the barplot of Absent and Present species\n    sns.barplot(x = ['Absent', 'Present'], y = class_counts, color = '#005477')\n    plt.title('Distribution of Species Presence/Absence')\n    plt.ylabel('Number of Sampling sites')\n    \n    # Add percent over each bar\n    total = len(y)\n    for i,count in enumerate(class_counts):\n        percentage = count/total * 100 \n        plt.text(i, count, f'{percentage:.1f}%', ha = 'center', va = 'bottom')\n    plt.show()\n\n\n\nFunction 3\nCreate a function that plots a confusion matrix of the predicted y values and true y values.\n\ndef plot_confusion_matrix(y_true, y_pred):\n    \n    # Create confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Create confusion matrix plot\n    plt.figure(figsize = (8,6))\n    sns.heatmap(cm, fmt = 'd', cmap = 'GnBu',annot = True,\n               xticklabels = ['Absent', 'Present'],\n               yticklabels = ['Absent', 'Present'])\n    plt.title('Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.show()\n    \n    \n    # Calculate and display metrics\n    \n    TP = cm[1,1]\n    TN = cm[0,0]\n    FP = cm[0,1]\n    FN = cm[1,0]\n    \n    print(f\"True positives (correctly predicted presence): {TP}\")\n    \n    \n    # Calculate accuracy + various metric \n    accuracy = (TP + TN) / (TP + TN + FP + FN)\n    sensitivity = TP/ (TP + FN)\n    specificity = TN / (TN + FP )\n    \n    \n    print(f\"\\nModel Performance Metrics:\")\n    print(f\"Accuracy: {accuracy:.3f}\")\n    print(f\"Sensitivity ( True positive rate): {sensitivity:.3f}\")\n    print(f\"Specificity ( True negative rate:) {specificity:.3f}\")\n\n\n\nFunction 4\nCreate a function that plots an ROC curve using the predicted y class probabilities and true y values.\n\ndef plot_roc_curve(y_test, y_pred_prob):\n    \n    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n    \n    roc_auc = auc(fpr, tpr)\n    \n    plt.figure(figsize = (8,6))\n    plt.plot(fpr, tpr, color = 'darkorange', lw =2, label = f'ROC Curve (AUC = {roc_auc:.2f})')\n    plt.plot([0,1], [0,1], color = 'navy', lw = 2, linestyle = '--',\n            label = 'Random Classifier (AUC = 0.5)')\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC curve: Species Presence Prediction')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n\n\nFunction 5\nCreate function that runs a logistic regression and outputs the three plots you created above.\n\ndef interactive_logistic_regression(presence_ratio = 0.3):\n    \n    # Generate data based on class imbalance from user\n    X,y = generate_species_data(presence_ratio = presence_ratio)\n    \n    # Plot class distribution\n    print(\"\\nClass Distribution\")\n    plot_class_distribution(y)\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.3, random_state = 42)\n    \n    # Train model\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    \n    # Make predict\n    y_pred = model.predict(X_test)\n    y_pred_prob = model.predict_proba(X_test)[:,1]\n    \n    # Plot confusion matrix\n    print(\"\\nConfusion matrix:\")\n    plot_confusion_matrix(y_test, y_pred)\n    \n    # Plot ROC curve\n    print(\"\\nROC Curve:\")\n    plot_roc_curve(y_test, y_pred_prob)\n\n\n\nFunction 6\nCreate a function that adds interactivity to function 5.\n\n# Create interactive widget\n\ndef generate_log_regression():\n    interact(interactive_logistic_regression, \n            presence_ratio = FloatSlider(min = .1, max = .9, step= .1, value = 0.3,\n                                        description = \"% Present\"))\ngenerate_log_regression()"
  },
  {
    "objectID": "discussion/week0.html",
    "href": "discussion/week0.html",
    "title": "week0",
    "section": "",
    "text": "This will contain content to support students in creating a notebook for the week’s section material.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "discussion/week0.html#test-section-page",
    "href": "discussion/week0.html#test-section-page",
    "title": "week0",
    "section": "",
    "text": "This will contain content to support students in creating a notebook for the week’s section material.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning in Environmental Science",
    "section": "",
    "text": "Image created using the Midjourney image generation tool"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Machine Learning in Environmental Science",
    "section": "Course Description",
    "text": "Course Description\nMachine learning is a field of inquiry devoted to understanding and building methods that “learn,” that is, methods that leverage data to improve performance on some set of tasks. In this course, we focus on the core concepts of machine learning that beginning ML researchers must know. We cover “classical machine learning” primarily using R and explore applications to environmental science. To understand broader concepts of artificial intelligence or deep learning, a strong fundamental knowledge of machine learning is indispensable."
  },
  {
    "objectID": "index.html#teaching-team",
    "href": "index.html#teaching-team",
    "title": "Machine Learning in Environmental Science",
    "section": "Teaching Team",
    "text": "Teaching Team\nInstructor: Mateo Robbins (mjrobbins@ucsb.edu)\nStudent hours: Tuesdays 10:45am (Bren 1424)\nTeaching Assistant: Annie Adams (aradams@ucsb.edu)\nStudent hours: Thursdays 11:00am (Bren 1520 - Oak Room)"
  },
  {
    "objectID": "index.html#important-links",
    "href": "index.html#important-links",
    "title": "Machine Learning in Environmental Science",
    "section": "Important Links",
    "text": "Important Links\n\nLink to full course syllabus"
  },
  {
    "objectID": "index.html#weekly-course-schedule",
    "href": "index.html#weekly-course-schedule",
    "title": "Machine Learning in Environmental Science",
    "section": "Weekly Course Schedule",
    "text": "Weekly Course Schedule\nLecture: TTh 9:30am - 10:45am (Bren 1424)\nSections: Th 1:00pm - 1:50pm or 2:00 - 2:50pm (Bren 3022)"
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "Machine Learning in Environmental Science",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThe goal of EDS 232 is to equip students with a strong foundation in the core concepts of machine learning. By the end of the course, students should be able to:\n\nExplain key machine learning concepts such as classification, regression, overfitting, and the trade-off in model complexity.\nIdentify and justify appropriate data preprocessing techniques and integrate them into machine learning pipelines.\nDemonstrate an intuitive understanding of common machine learning algorithms.\nBuild supervised machine learning pipelines using Python and scikit-learn on real-world datasets.\nApply best practices for machine learning development so that your models generalize to data and tasks in the real world. Measure and contrast the performance of various models"
  },
  {
    "objectID": "index.html#course-requirements",
    "href": "index.html#course-requirements",
    "title": "Machine Learning in Environmental Science",
    "section": "Course Requirements",
    "text": "Course Requirements\n\nComputing\n\nMinimum MEDS device requirements\n\n\n\nTextbook\n\nIntro to Statistical Learning with Python (ISL)"
  },
  {
    "objectID": "index.html#course-topics",
    "href": "index.html#course-topics",
    "title": "Machine Learning in Environmental Science",
    "section": "Course Topics",
    "text": "Course Topics\n\n\n\nWeek #\nDates\nLecture\nReading\n\n\n1\n1/7, 1/9\nIntroduction\nLinear Regression and ML Modeling Fundamentals I\nISL Ch. 1, 2.1\n\n\n2\n1/14, 1/16\nRegularized Regression and ML Modeling Fundamentals II\nISL Ch. 5.1.1-5.1.4, 6.2\n\n\n3\n1/21, 1/23\nLogistic Regression, Classification\nISL Ch. 4.1-4.4\n\n\n4\n1/28, 1/30\nK-nearest neighbors, Decision Trees\nISL Ch. 2.2.3, 8.1\n\n\n5\n2/3, 2/6\nBagging, Random Forest\nISL Ch. 8.2.1, 8.2.2\n\n\n6\n2/11, 2/13\nGuest Lecture\n\n\n\n7\n2/18, 2/20\nGradient Boosting\n\n\n\n8\n2/25, 2/27\nClustering\n\n\n\n9\n3/4, 3/6\nSVM\n\n\n\n10\n3/11, 3/13\nKaggle"
  },
  {
    "objectID": "labs/lab_instructions.html#initial-repository-setup",
    "href": "labs/lab_instructions.html#initial-repository-setup",
    "title": "Welcome to our weekly machine learning labs!",
    "section": "Initial Repository Setup",
    "text": "Initial Repository Setup\n\nStep 1: Fork the Repository\n\nNavigate to the following Repository: https://github.com/annieradams/EDS232-labs\nFork the Repository: Click the “Fork” button located at the top right corner of the page. This creates a copy of the repository in your GitHub account.\n\n\n\nStep 2: Clone Your Fork\n\nCopy the URL of Your Fork: On your fork’s GitHub page, click the “Code” button and copy the URL provided.\nClone the Repository: Start a new Jupyter Lab Session on Workbench 1 and run the following command (replace URL_OF_YOUR_FORK with the URL you just copied):\n\ngit clone URL_OF_YOUR_FORK\n\n\nStep 3: Configure your remote branch\n\nAdd upstream remote branch.: Change your directory to the new repository. Once there, copy the following line into your terminal:\n\ngit remote add upstream https://github.com/annieradams/EDS232-labs.git\nTo verify the new upstream repository you have specified for your fork, type\ngit remote -v \nYou should see the URL for your fork as origin, and the URL for the upstream repository as upstream."
  },
  {
    "objectID": "labs/lab_instructions.html#weekly-fetching-to-get-new-labs",
    "href": "labs/lab_instructions.html#weekly-fetching-to-get-new-labs",
    "title": "Welcome to our weekly machine learning labs!",
    "section": "Weekly Fetching to get new labs",
    "text": "Weekly Fetching to get new labs\nTo ensure you have the latest lab materials each week, make sure you are at the correct directoy and copy the following code in the terminal:\ngit pull upstream main  # or master if the main branch is named master\nThis command will fetch the latest updates from the upstream and merge them into your current branch.\n\nYou are now ready to start working on this week’s lab!!"
  },
  {
    "objectID": "discussion/week1.html",
    "href": "discussion/week1.html",
    "title": "Discussion 1",
    "section": "",
    "text": "In this week’s discussion section, we will be using the same dataset from our weekly lab - Water characteristics in the Hudson River after Hurricane Irene. However, rather than looking at a single predictor variable, we are going to add more! Can we improve our model if we add more variables?? Let’s find out."
  },
  {
    "objectID": "discussion/week1.html#introduction",
    "href": "discussion/week1.html#introduction",
    "title": "Discussion 1",
    "section": "",
    "text": "In this week’s discussion section, we will be using the same dataset from our weekly lab - Water characteristics in the Hudson River after Hurricane Irene. However, rather than looking at a single predictor variable, we are going to add more! Can we improve our model if we add more variables?? Let’s find out."
  },
  {
    "objectID": "discussion/week1.html#data-loading",
    "href": "discussion/week1.html#data-loading",
    "title": "Discussion 1",
    "section": "Data Loading",
    "text": "Data Loading\nAccess the same .xlsx file we used in lab this week. If you lost access to it, you can find the data here. Instead of looking at only the dissolved oxygen and turbidity data this time, we are also going to read in data on rainfall. Read in each of these sheets on the excel sheet as its own dataframe. Load the following libraries:\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact\nimport ipywidgets as widgets\nfrom ipywidgets import interact, FloatSlider\nfrom IPython.display import display, clear_output\n\n\n# Load the data\nfp = '../data/Hurricane_Irene_Hudson_River.xlsx'\ndo_data = pd.read_excel(fp, sheet_name = 5).drop(['Piermont D.O. (ppm)'], axis = 1)\nrainfall_data = pd.read_excel(fp, sheet_name='Rainfall').drop(['Piermont  Rainfall Daily Accumulation (Inches)'], axis = 1)\nturbidity_data = pd.read_excel(fp, sheet_name='Turbidity').drop(['Piermont Turbidity in NTU'], axis = 1)"
  },
  {
    "objectID": "discussion/week1.html#data-wrangling",
    "href": "discussion/week1.html#data-wrangling",
    "title": "Discussion 1",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nPerform the following data wrangling steps to get our data ready for our model.\n\nMerge the three dataframes together. While merging, or after, drop all columns for the Piedmont location.\nUpdate the column names to be shorter and not have spaces. Use snake case.\nMake your date column a datetime obect.\nSet the data as the index for the merged dataframe.\n\n\n# Merge the two datasets on date\ndata = rainfall_data.merge(turbidity_data, on = 'Date Time (ET)')\ndata = data.merge(do_data, on = 'Date Time (ET)')\ndata.head()\n\n# Update the column names \ndata.columns = ['date', 'albany_rainfall', 'norrie_rainfall', 'albany_turbidity', 'norrie_turbidity','albany_do', 'norrie_do']\n\n# Convert data to datetime format and set it as index\ndata['date'] = pd.to_datetime(data['date'])\n\n# Update index\ndata.set_index('date', inplace=True)"
  },
  {
    "objectID": "discussion/week1.html#multiple-linear-regression",
    "href": "discussion/week1.html#multiple-linear-regression",
    "title": "Discussion 1",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nNow that our data is cleaned, let’s do the following to carry out a multiple linear regression.\n\nDefine your predictors and target variables.\nSplit the data into training and testing sets\nCreate and fit the model\nPredict and Evaluate your model\n\n\n# Define predictors and the target variable\nX = data[['albany_rainfall', 'norrie_rainfall', 'albany_do', 'norrie_do']]  # Adjust as needed\ny = data['albany_turbidity']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Create and fit the model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = model.predict(X_test)\nprint(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred))}\")\nprint(f\"R-squared: {r2_score(y_test, y_pred)}\")\n\nRMSE: 187.03290519070686\nR-squared: 0.6382523355891789"
  },
  {
    "objectID": "discussion/week1.html#create-a-widget-for-updating-the-predictor-and-target-variables.",
    "href": "discussion/week1.html#create-a-widget-for-updating-the-predictor-and-target-variables.",
    "title": "Discussion 1",
    "section": "Create a Widget for updating the predictor and target variables.",
    "text": "Create a Widget for updating the predictor and target variables.\n\nCreate the four different pieces to the widget: the predictor selector, the target selector, the evaluate button, and the output\nWrap our worfklow into a function called evaluate_model(). This function will run a linear regression model based on what the user selects as predictors and the outcome variable. It will print the \\(R^2\\), MSE, and a scatterplot of the actual versus predicted target variable.\nCreate a warning for your widget to ensure that the user does not select the same variable as both a predictor variable and a target variable.\nPlay around with your widget and see how your \\(R^2\\) changes based on your selected variables!\n\n\n# Create a widget for selecting predictors\npredictor_selector = widgets.SelectMultiple(\n    options=data.columns, # Options for predictor: columns of data\n    value=[data.columns[0]],  # Default selected: 1st column of data (albany_rainfall)\n    description='Predictors' # Name the predictor selection\n)\n\n# Create a dropdown for selecting the target variable\ntarget_selector = widgets.Dropdown(\n    options=data.columns, # Options for predictor: columns of data\n    value=data.columns[1],  # Default selected: 2nd column of data (norrie_rainfall)\n    description='Target',\n)\n\n# Create button to evaluate the model\nevaluate_button = widgets.Button(description=\"Evaluate Model\")\n\n# Output widget to display results\noutput = widgets.Output()\n\n# Define the function to handle button clicks\ndef evaluate_model(b):\n    with output:\n        clear_output(wait=True) # Clear previous displayed output before running\n        \n        # Make sure the target variable is not also a predictor variable\n        selected_predictors = [item for item in predictor_selector.value] # Pull out predictor values selected by user\n        if target_selector.value in selected_predictors: # Make sure target variable is not also a predictor variable\n            print(\"Target variable must not be in the predictors.\")\n            return\n        \n        # Assign X and y variables\n        X = data[selected_predictors]\n        y = data[target_selector.value]\n        \n        # Split data into training and testing sets\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n        \n        # Create and fit the model\n        model = LinearRegression()\n        model.fit(X_train, y_train)\n        \n        # Predict and calculate R^2 and MSE\n        y_pred = model.predict(X_test)\n        r2 = r2_score(y_test, y_pred)\n        mse = mean_squared_error(y_test, y_pred)\n        \n        # Display the R^2 score and MSE\n        print(f\"R^2: {r2:.4f}\")\n        print(f\"MSE: {mse:.4f}\")\n\n\n        # Create a scatter plot of y test vs predicted y\n        plt.scatter(y_test, y_pred) \n        plt.xlabel('Actual') \n        plt.ylabel('Predicted') \n        plt.title('Actual vs Predicted') \n        plt.show() \n\n\n# Display the widgets and connect the button to the function\ndisplay(predictor_selector, target_selector, evaluate_button, output)\nevaluate_button.on_click(evaluate_model)"
  },
  {
    "objectID": "discussion/week2.html",
    "href": "discussion/week2.html",
    "title": "Discussion 2",
    "section": "",
    "text": "In this week’s discussion section, we will create some interactive plots to better undertsand how lasso and ridge regression are at work. To do so, we will use synthesized data that is made with the intention of better understanding how ridge and lasso regression are different based on the relationship of your parameters. It is important to note that your results with real data may look very different - unlike this notebook, the real world data you will be working with was not made to better understand regression models."
  },
  {
    "objectID": "discussion/week2.html#introduction",
    "href": "discussion/week2.html#introduction",
    "title": "Discussion 2",
    "section": "",
    "text": "In this week’s discussion section, we will create some interactive plots to better undertsand how lasso and ridge regression are at work. To do so, we will use synthesized data that is made with the intention of better understanding how ridge and lasso regression are different based on the relationship of your parameters. It is important to note that your results with real data may look very different - unlike this notebook, the real world data you will be working with was not made to better understand regression models."
  },
  {
    "objectID": "discussion/week2.html#data-loading",
    "href": "discussion/week2.html#data-loading",
    "title": "Discussion 2",
    "section": "Data Loading",
    "text": "Data Loading\nCopy the code below to load the neessary libraries genereate the data we will use. Read the comments to on each feature to get an idea of the relationship between variables.\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Lasso, Ridge\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom ipywidgets import interact, FloatLogSlider\n\n# Generate data\nnp.random.seed(42)\nn_samples = 200\nX = np.zeros((n_samples, 6))\nX[:, 0] = np.random.normal(0, 1, n_samples)  # X1 - Important feature\nX[:, 1] = np.random.normal(0, 1, n_samples)  # X2 -  Important feature\nX[:, 2] = X[:, 0] + np.random.normal(0, 0.1, n_samples)  # Correlated with X1\nX[:, 3] = X[:, 1] + np.random.normal(0, 0.1, n_samples)  # Correlated with X2\nX[:, 4] = np.random.normal(0, 0.1, n_samples)  # Noise\nX[:, 5] = np.random.normal(0, 0.1, n_samples)  # Noise\n\ny = 3 * X[:, 0] + 2 * X[:, 1] + 0.5 * X[:, 2] + np.random.normal(0, 0.1, n_samples)"
  },
  {
    "objectID": "discussion/week2.html#regression",
    "href": "discussion/week2.html#regression",
    "title": "Discussion 2",
    "section": "Regression",
    "text": "Regression\nNow that you have your data, do the following:\n\nSplit your data into training and testing.\n\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\nCreate and fit a ridge regression\n\n\n# Create and fit Ridge regression model\nridge_model = Ridge()\nridge_model.fit(X_train, y_train)\nridge_predictions = ridge_model.predict(X_test)\n\n\nCalculate the MSE and \\(R^2\\) for your ridge regression.\n\n\n# Calculate MSE and R^2 for Ridge regression\nridge_rmse = np.sqrt(mean_squared_error(y_test, ridge_predictions))\nridge_r2 = r2_score(y_test, ridge_predictions)\nprint(\"Ridge Regression RMSE:\", ridge_rmse)\nprint(\"Ridge Regression R²:\", ridge_r2)\n\nRidge Regression RMSE: 0.14410020171824725\nRidge Regression R²: 0.9984722762470866\n\n\n\nCreate and fit a lasso model.\n\n\n# Create and fit Lasso regression model\nlasso_model = Lasso() \nlasso_model.fit(X_train, y_train)\nlasso_predictions = lasso_model.predict(X_test)\n\n\nCalculate the MSE and \\(R^2\\) for your lasso model.\n\n\n# Calculate RMSE and R^2 for Lasso regression\nlasso_rmse = np.sqrt(mean_squared_error(y_test, lasso_predictions))\nlasso_r2 = r2_score(y_test, lasso_predictions)\nprint(\"Lasso Regression RMSE:\", lasso_rmse)\nprint(\"Lasso Regression R²:\", lasso_r2)\n\nLasso Regression RMSE: 1.2984978990079017\nLasso Regression R²: 0.8759496036905758"
  },
  {
    "objectID": "discussion/week2.html#visualizing-ridge-vs-regression",
    "href": "discussion/week2.html#visualizing-ridge-vs-regression",
    "title": "Discussion 2",
    "section": "Visualizing Ridge vs Regression",
    "text": "Visualizing Ridge vs Regression\n\nCreate a plot that looks at the alpha against the MSE for both lasso and ridge regression.\n\n\n# Visualize alphas against RMSE for lasso and ridge\n\n# Initialize lists to append data into\nrmse_lasso = []\nrmse_ridge = []\n\n# Define alpha values to iterate over\nalphas = [0.1,1,10]\n\n# Create and fit a lasso and ridge model for each predefined alpha\nfor alpha in alphas:\n    lasso = Lasso(alpha=alpha)\n    ridge = Ridge(alpha=alpha)\n    \n    lasso.fit(X_train, y_train)\n    ridge.fit(X_train, y_train)\n\n    # Calculate rmse for both models\n    rmse_lasso.append(np.sqrt(mean_squared_error(y_test, lasso.predict(X_test))))\n    rmse_ridge.append(np.sqrt(mean_squared_error(y_test, ridge.predict(X_test))))\n\n# Create plot of MSE again alpha values \nplt.figure(figsize=(10, 5))\nplt.plot(alphas, rmse_lasso, label='Lasso MSE')\nplt.plot(alphas, rmse_ridge, label='Ridge MSE')\nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('Root Mean Squared Error')\nplt.title('RMSE vs. Alpha for Lasso and Ridge Regression')\nplt.legend()\nplt.show()\n\n\n\n\n\nCreate an interactive plot (for both lasso and ridge) that allows you to adjust alpha to see how the actual vs predicted values are changing.\n\n\n# Create function to run model and create plot\n\ndef update_alphas(alpha, model_type):\n\n    # Condition to allow user to select different models\n    if model_type == 'Lasso':\n        model = Lasso(alpha=alpha)\n    else:\n        model = Ridge(alpha=alpha)\n\n    # Fit and predict model\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    # Calculate model metrics\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    r2 = r2_score(y_test, y_pred)\n\n    # Create plot of predicted values against actual values with line of best fit\n    plt.figure(figsize=(10, 5))\n    # Add predicted and actual values\n    plt.scatter(y_test, y_pred, color='blue', alpha=0.5, label=f'Predictions (alpha={alpha})')\n    # Add line of best fit\n    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n    plt.title(f'{model_type} Regression: Predictions vs Actual (alpha={alpha})')\n    plt.xlabel('Actual Values')\n    plt.ylabel('Predicted Values')\n    plt.legend()\n    # Adjust the position and aesthetics of the metric box\n    plt.figtext(0.5, -0.05, f'MSE: {rmse:.2f}, R²: {r2:.2f}', ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n    plt.show()\n\n# Create interactive widgets\n\n# Create alpha slider for choosing alpha value\nalpha_slider = FloatLogSlider(value= 0 , base=10, min=-3, max=3, step=0.1, description='Pick an Alpha!')\n\n# Create model selector for picking which model user wants to look at\nmodel_selector = {'Lasso': 'Lasso', 'Ridge': 'Ridge'}\n\n# Combine two widgets with model/plot output\ninteract(update_alphas, alpha=alpha_slider, model_type=model_selector)\n\n\n\n\n&lt;function __main__.update_alphas(alpha, model_type)&gt;\n\n\n\nCreate three different bar plots with the following guidelines: Each plot should represent a different alpha value: Alpha = 0.1, Alpha = 1, Alpha = 10 Each plot should show how both the ridge and lasso model performed The y axis should represent the six different variables: X1, X2, X1_corr, X2_corr, Noise1, Noise2. The y axis should represent the coefficients\n\n\n# Define alpha values to iterate over\nalphas = [0.1, 1.0, 10.0]\ndata = []\n\n# Create and fit ridge and lasso models and store coefficients in a new dataframe\nfor alpha in alphas:\n    ridge = Ridge(alpha=alpha).fit(X_train, y_train)\n    lasso = Lasso(alpha=alpha).fit(X_train, y_train)\n    data.append(pd.DataFrame({\n        'Ridge': ridge.coef_, # coef has as many indexes as there are variables\n        'Lasso': lasso.coef_\n    }, index=['X1', 'X2', 'X1_corr', 'X2_corr', 'Noise1', 'Noise2'])) # create feature names in new dataframe\n\n\n# Create barplot to visualize how coefficients change across alpha values and models\nfig, axes = plt.subplots(1, 3, figsize=(12, 4), sharey=True) \nfor i, df in enumerate(data): \n    df.plot.bar(ax=axes[i], width= 0.8)\n    axes[i].set_title(f'Alpha = {alphas[i]}')\n    axes[i].set_xticklabels(df.index, rotation=45)\n    \nplt.show()"
  },
  {
    "objectID": "discussion/week6.html",
    "href": "discussion/week6.html",
    "title": "Discussion 6",
    "section": "",
    "text": "In this week’s discussion section, we will use a data with few NAs and intentionally add more NAs to it. We are going to run different imputation strategies on our newly “NA-ed” dataset, and see which performs best. Normally, you would never know how your imputation is actually performing, but this excercise will allow us to look under the hood a bit at how different imputation strategies perform differently. Once we find which imputation strategy works best, we will run a random forest on both the original data, as well as our newly imputed data. Which do you think will perform better??"
  },
  {
    "objectID": "discussion/week6.html#data",
    "href": "discussion/week6.html#data",
    "title": "Discussion 6",
    "section": "Data",
    "text": "Data\nThis week, we will be working with mushroom data! This dataset from the UCI Machine Learning Repository includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. Our target variable will be, poisonous, a categorical outcome variable classifying the mushroom as poisonous or not. We will include 22 features in our dataset that all relate to mushroom characteristics- such as cap-cut, cap-surface, bruises, and odor."
  },
  {
    "objectID": "discussion/week6.html#excercise",
    "href": "discussion/week6.html#excercise",
    "title": "Discussion 6",
    "section": "Excercise",
    "text": "Excercise\n\nImport Libraries\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.metrics import mean_squared_error, accuracy_score, r2_score\nfrom ucimlrepo import fetch_ucirepo \n\n\n\nLoad Data\n\n# Fetch dataset \nmushroom = fetch_ucirepo(id=73) \n  \n# Save data as X and y variables\nX = mushroom.data.features \ny = np.ravel(mushroom.data.targets)\n\n# Expand dataframe columns and look at view dataframe\npd.set_option('display.max_columns', None)\nX.head()\n\n\n\n\n\n\n\n\ncap-shape\ncap-surface\ncap-color\nbruises\nodor\ngill-attachment\ngill-spacing\ngill-size\ngill-color\nstalk-shape\nstalk-root\nstalk-surface-above-ring\nstalk-surface-below-ring\nstalk-color-above-ring\nstalk-color-below-ring\nveil-type\nveil-color\nring-number\nring-type\nspore-print-color\npopulation\nhabitat\n\n\n\n\n0\nx\ns\nn\nt\np\nf\nc\nn\nk\ne\ne\ns\ns\nw\nw\np\nw\no\np\nk\ns\nu\n\n\n1\nx\ns\ny\nt\na\nf\nc\nb\nk\ne\nc\ns\ns\nw\nw\np\nw\no\np\nn\nn\ng\n\n\n2\nb\ns\nw\nt\nl\nf\nc\nb\nn\ne\nc\ns\ns\nw\nw\np\nw\no\np\nn\nn\nm\n\n\n3\nx\ny\nw\nt\np\nf\nc\nn\nn\ne\ne\ns\ns\nw\nw\np\nw\no\np\nk\ns\nu\n\n\n4\nx\ns\ng\nf\nn\nf\nw\nb\nk\nt\ne\ns\ns\nw\nw\np\nw\no\ne\nn\na\ng\n\n\n\n\n\n\n\n\n\nEncoding Data\n\n# Factorize all columns\nfor col in X.columns: \n    X.loc[:, col] = pd.factorize(X[col], sort = True)[0]\n\n# View first few rows of encoded data\nX.iloc[0:5, 0:5]\n\n\n\n\n\n\n\n\ncap-shape\ncap-surface\ncap-color\nbruises\nodor\n\n\n\n\n0\n5\n2\n4\n1\n6\n\n\n1\n5\n2\n9\n1\n0\n\n\n2\n0\n2\n8\n1\n3\n\n\n3\n5\n3\n8\n1\n6\n\n\n4\n5\n2\n3\n0\n5\n\n\n\n\n\n\n\n\n\nTime to impute!\nDoes our dataset have any missing values? Lets check!\n\n# Check for NAs\nX.isna().sum()\n\ncap-shape                   0\ncap-surface                 0\ncap-color                   0\nbruises                     0\nodor                        0\ngill-attachment             0\ngill-spacing                0\ngill-size                   0\ngill-color                  0\nstalk-shape                 0\nstalk-root                  0\nstalk-surface-above-ring    0\nstalk-surface-below-ring    0\nstalk-color-above-ring      0\nstalk-color-below-ring      0\nveil-type                   0\nveil-color                  0\nring-number                 0\nring-type                   0\nspore-print-color           0\npopulation                  0\nhabitat                     0\ndtype: int64\n\n\nWe are going to randomly assign observations in our dataset to be missing, and then see which imputation methods perform best by comparing their results to our actual dataset. Let’s randomly assign NA observations throughout our dataset. We will create a copy of our dataframe and call it X_Na.\n\n# Create copy of X variables\nX_Na = X.copy()\n\n\n# Assign 10% of new dataframe with NA values\nfor col in X_Na.columns: \n    X_Na.loc[X_Na.sample(frac = 0.1).index, col] = np.nan\n\n\n# Check to make sure there are missing values\nX_Na.isna().sum()\n\ncap-shape                   812\ncap-surface                 812\ncap-color                   812\nbruises                     812\nodor                        812\ngill-attachment             812\ngill-spacing                812\ngill-size                   812\ngill-color                  812\nstalk-shape                 812\nstalk-root                  812\nstalk-surface-above-ring    812\nstalk-surface-below-ring    812\nstalk-color-above-ring      812\nstalk-color-below-ring      812\nveil-type                   812\nveil-color                  812\nring-number                 812\nring-type                   812\nspore-print-color           812\npopulation                  812\nhabitat                     812\ndtype: int64\n\n\nNow that we have our dataset with missing values, let’s impute!\n\nImputation method #1: Filling NA values with the mode\n\n# Impute with mode\nX_mode_impute = X_Na.fillna(X_Na.mode().iloc[0])\n\n# Check to make sure there are no NAs\nX_mode_impute.isna().sum()\n\ncap-shape                   0\ncap-surface                 0\ncap-color                   0\nbruises                     0\nodor                        0\ngill-attachment             0\ngill-spacing                0\ngill-size                   0\ngill-color                  0\nstalk-shape                 0\nstalk-root                  0\nstalk-surface-above-ring    0\nstalk-surface-below-ring    0\nstalk-color-above-ring      0\nstalk-color-below-ring      0\nveil-type                   0\nveil-color                  0\nring-number                 0\nring-type                   0\nspore-print-color           0\npopulation                  0\nhabitat                     0\ndtype: int64\n\n\n\n\nImputation method #2: Filling NA values with the median using SimpleImputer\n\n# Impute with median (using SimpleImputer) \nmedian_impute = SimpleImputer(strategy = 'median')\nX_median_impute = median_impute.fit_transform(X_Na)\nX_median_impute = pd.DataFrame(X_median_impute, columns = X.columns)\n\n# Check to make sure there are no NAs\nX_median_impute.isna().sum()\n\ncap-shape                   0\ncap-surface                 0\ncap-color                   0\nbruises                     0\nodor                        0\ngill-attachment             0\ngill-spacing                0\ngill-size                   0\ngill-color                  0\nstalk-shape                 0\nstalk-root                  0\nstalk-surface-above-ring    0\nstalk-surface-below-ring    0\nstalk-color-above-ring      0\nstalk-color-below-ring      0\nveil-type                   0\nveil-color                  0\nring-number                 0\nring-type                   0\nspore-print-color           0\npopulation                  0\nhabitat                     0\ndtype: int64\n\n\n\n\nImputation method #3: Filling NA values with KNN Imputer\n\n# Impute with KNN Imputer\nknn_impute = KNNImputer(n_neighbors = 20)\nX_knn_impute = knn_impute.fit_transform(X_Na)\nX_knn_impute = pd.DataFrame(X_knn_impute, columns = X_Na.columns)\n\n# Check to make sure there are no NAs\nX_knn_impute.isna().sum()\n\ncap-shape                   0\ncap-surface                 0\ncap-color                   0\nbruises                     0\nodor                        0\ngill-attachment             0\ngill-spacing                0\ngill-size                   0\ngill-color                  0\nstalk-shape                 0\nstalk-root                  0\nstalk-surface-above-ring    0\nstalk-surface-below-ring    0\nstalk-color-above-ring      0\nstalk-color-below-ring      0\nveil-type                   0\nveil-color                  0\nring-number                 0\nring-type                   0\nspore-print-color           0\npopulation                  0\nhabitat                     0\ndtype: int64\n\n\n\n\nNow that we have four different dataframes with four different imputation methods, lets see which best captured our real data!We can do this using the mean squared error!\n\n# Calculate imputation accuracy using mean squared error\nmse_mode = mean_squared_error(X, X_mode_impute)\nmse_median = mean_squared_error(X, X_median_impute)\nmse_knn = mean_squared_error(X, X_knn_impute)\n\n\n# Report results\nprint(f\"Mode imputation performance: {mse_mode}\")\nprint(f\"Median Imputation performance: {mse_median}\")\nprint(f\"KNN Imputation performance: {mse_knn}\")\n\nMode imputation performance: 0.4621547826865405\nMedian Imputation performance: 0.25424108141981117\nKNN Imputation performance: 0.10042833243811827\n\n\n\n# Calculate imputation accuracy using R2\nr2_mode = r2_score(X, X_mode_impute)\nr2_median = r2_score(X, X_median_impute)\nr2_knn = r2_score(X, X_knn_impute)\n\n\n# Report results\nprint(f\"Mode imputation performance: {r2_mode}\")\nprint(f\"Median Imputation performance: {r2_median}\")\nprint(f\"KNN Imputation performance: {r2_knn}\")\n\nMode imputation performance: 0.850983324758015\nMedian Imputation performance: 0.8851326770563549\nKNN Imputation performance: 0.9687095552578344\n\n\nIt looks like our KNN Imputation was the most successfull in imputing NAs! Let’s run a random forest with our actual data, and our KNN imputed data to see how/if they differ!\n\n\n\nRandom Forest Classifier with original data\n\n# Split actual data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n\n\n# Number of features to include for tuning\nnum_features = [1,4,7,10,13,16,19,22]\naccuracy = []\n\nfor feature in num_features: \n    rf_classifier = RandomForestClassifier(\n        n_estimators = 50, \n        max_depth = 3, \n        random_state = 42, \n        max_features = feature\n    )\n    \n    # Train model\n    rf_classifier.fit(X_train, y_train)\n    \n    # Predict and evaluate \n    y_pred = rf_classifier.predict(X_test)\n    rf_accuracy = accuracy_score(y_test, y_pred)\n    accuracy.append(rf_accuracy)\n    print(f\"Number of features: {feature}; Random Forest Accuracy: {rf_accuracy}\")\n\nNumber of features: 1; Random Forest Accuracy: 0.916735028712059\nNumber of features: 4; Random Forest Accuracy: 0.9848236259228876\nNumber of features: 7; Random Forest Accuracy: 0.9868744872846595\nNumber of features: 10; Random Forest Accuracy: 0.9835931091058244\nNumber of features: 13; Random Forest Accuracy: 0.9823625922887613\nNumber of features: 16; Random Forest Accuracy: 0.9860541427399507\nNumber of features: 19; Random Forest Accuracy: 0.9819524200164069\nNumber of features: 22; Random Forest Accuracy: 0.9577522559474979\n\n\n\n\nRandom Forest Classifier with imputed data:\n\n# Split imputed data \nX_train, X_test, y_train, y_test = train_test_split(X_knn_impute, y, test_size = 0.3, random_state = 42)\n\n\n# Number of features to include for tuning\n# Number of features to include for tuning\nnum_features = [1,4,7,10,13,16,19,22]\naccuracy = []\n\nfor feature in num_features: \n    rf_classifier = RandomForestClassifier(\n        n_estimators = 50, \n        max_depth = 3, \n        random_state = 42, \n        max_features = feature\n    )\n    \n    # Train model\n    rf_classifier.fit(X_train, y_train)\n    \n    # Predict and evaluate \n    y_pred = rf_classifier.predict(X_test)\n    rf_accuracy = accuracy_score(y_test, y_pred)\n    accuracy.append(rf_accuracy)\n    print(f\"Number of features: {feature}; Random Forest Accuracy: {rf_accuracy}\")\n\nNumber of features: 1; Random Forest Accuracy: 0.920836751435603\nNumber of features: 4; Random Forest Accuracy: 0.9774405250205086\nNumber of features: 7; Random Forest Accuracy: 0.9840032813781788\nNumber of features: 10; Random Forest Accuracy: 0.9840032813781788\nNumber of features: 13; Random Forest Accuracy: 0.9844134536505332\nNumber of features: 16; Random Forest Accuracy: 0.9762100082034455\nNumber of features: 19; Random Forest Accuracy: 0.9680065627563577\nNumber of features: 22; Random Forest Accuracy: 0.9573420836751435"
  }
]