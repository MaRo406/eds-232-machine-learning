[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning in Environmental Science",
    "section": "",
    "text": "Image created using the Midjourney image generation tool"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Machine Learning in Environmental Science",
    "section": "Course Description",
    "text": "Course Description\nMachine learning is a field of inquiry devoted to understanding and building methods that “learn,” that is, methods that leverage data to improve performance on some set of tasks. In this course, we focus on the core concepts of machine learning that beginning ML researchers must know. We cover “classical machine learning” primarily using R and explore applications to environmental science. To understand broader concepts of artificial intelligence or deep learning, a strong fundamental knowledge of machine learning is indispensable."
  },
  {
    "objectID": "index.html#teaching-team",
    "href": "index.html#teaching-team",
    "title": "Machine Learning in Environmental Science",
    "section": "Teaching Team",
    "text": "Teaching Team\nInstructor: Mateo Robbins (mjrobbins@ucsb.edu)\nStudent hours: Tuesdays 10:45am (Bren 1424)\nTeaching Assistant: Annie Adams (aradams@ucsb.edu)\nStudent hours: Thursdays 11:00am (Bren 1520 - Oak Room)"
  },
  {
    "objectID": "index.html#important-links",
    "href": "index.html#important-links",
    "title": "Machine Learning in Environmental Science",
    "section": "Important Links",
    "text": "Important Links\n\nLink to full course syllabus"
  },
  {
    "objectID": "index.html#weekly-course-schedule",
    "href": "index.html#weekly-course-schedule",
    "title": "Machine Learning in Environmental Science",
    "section": "Weekly Course Schedule",
    "text": "Weekly Course Schedule\nLecture: TTh 9:30am - 10:45am (Bren 1424)\nSections: Th 1:00pm - 1:50pm or 2:00 - 2:50pm (Bren 3022)"
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "Machine Learning in Environmental Science",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThe goal of EDS 232 is to equip students with a strong foundation in the core concepts of machine learning. By the end of the course, students should be able to:\n\nExplain key machine learning concepts such as classification, regression, overfitting, and the trade-off in model complexity.\nIdentify and justify appropriate data preprocessing techniques and integrate them into machine learning pipelines.\nDemonstrate an intuitive understanding of common machine learning algorithms.\nBuild supervised machine learning pipelines using Python and scikit-learn on real-world datasets.\nApply best practices for machine learning development so that your models generalize to data and tasks in the real world. Measure and contrast the performance of various models"
  },
  {
    "objectID": "index.html#course-requirements",
    "href": "index.html#course-requirements",
    "title": "Machine Learning in Environmental Science",
    "section": "Course Requirements",
    "text": "Course Requirements\n\nComputing\n\nMinimum MEDS device requirements\n\n\n\nTextbook\n\nIntro to Statistical Learning with Python (ISL)"
  },
  {
    "objectID": "index.html#course-topics",
    "href": "index.html#course-topics",
    "title": "Machine Learning in Environmental Science",
    "section": "Course Topics",
    "text": "Course Topics\n\n\n\nWeek #\nDates\nLecture\nReading\n\n\n1\n1/7, 1/9\nIntroduction\nLinear Regression and ML Modeling Fundamentals I\nISL Ch. 1, 2.1\n\n\n2\n1/14, 1/16\nRegularized Regression and ML Modeling Fundamentals II\nISL Ch. 5.1.1-5.1.4, 6.2\n\n\n3\n1/21, 1/23\nLogistic Regression, Classification\nISL Ch. 4.1-4.4\n\n\n4\n1/28, 1/30\nK-nearest neighbors, Decision Trees\nISL Ch. 2.2.3, 8.1\n\n\n5\n2/3, 2/6\nBagging, Random Forest\nISL Ch. 8.2.1, 8.2.2\n\n\n6\n2/11, 2/13\nGuest Lecture\n\n\n\n7\n2/18, 2/20\nGradient Boosting\n8.2.3, 8.2.5, 10.7.2\n\n\n8\n2/25, 2/27\nClustering\n12.4\n\n\n9\n3/4, 3/6\nSVM\n9.2\n\n\n10\n3/11, 3/13\nKaggle"
  },
  {
    "objectID": "discussion/week4.html",
    "href": "discussion/week4.html",
    "title": "Discussion 4",
    "section": "",
    "text": "In this week’s discussion section, we will run and tune a KNN and Decision Tree model. We will see how our models perform differently, as well as attempt to improve them! Given the variables listed below, we are going to build our model to predict which National Park a given taxon is part of."
  },
  {
    "objectID": "discussion/week4.html#time-to-dive-in-to-our-models",
    "href": "discussion/week4.html#time-to-dive-in-to-our-models",
    "title": "Discussion 4",
    "section": "Time to dive in to our models!",
    "text": "Time to dive in to our models!\n\n1. Prepare the data\nRead in the national park data and prepare your data from your models. We want our target variable to be ParkName, and all other variables in the table above to be our features. Then, split and scale the data.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import  accuracy_score\nfrom sklearn.tree import plot_tree\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Read in national park data\nnp_df = pd.read_csv('../data/national_park_species.csv')\nnp_df = np_df.drop(np_df.columns[0], axis=1)\npd.set_option('display.max_columns', None)\nnp_df.head()\n\n\n\n\n\n\n\n\nParkCode\nParkName\nCategoryName\nOrder\nFamily\nTaxonRecordStatus\nSciName\nCommonNames\nParkAccepted\nSensitive\nRecordStatus\nOccurrence\nOccurrenceTags\nNativeness\nNativenessTags\nAbundance\nNPSTags\nParkTags\nReferences\nObservations\nVouchers\nExternalLinks\nTEStatus\nStateStatus\nOzoneSensitiveStatus\nGRank\nSRank\n\n\n\n\n0\nACAD\nAcadia National Park\nMammal\nArtiodactyla\nCervidae\nActive\nAlces alces\nMoose\nTrue\nFalse\nApproved\nPresent\nNaN\nNative\nNaN\nRare\nResident\nNaN\n11\n1\n0\nNaN\n50\nNaN\nNaN\nG5\nME: S5\n\n\n1\nACAD\nAcadia National Park\nMammal\nArtiodactyla\nCervidae\nActive\nOdocoileus virginianus\nNorthern White-tailed Deer, Virginia Deer, Whi...\nTrue\nFalse\nApproved\nPresent\nNaN\nNative\nNaN\nAbundant\nNaN\nNaN\n20\n0\n0\nNaN\n50\nNaN\nNaN\nG5\nME: S5\n\n\n2\nACAD\nAcadia National Park\nMammal\nCarnivora\nCanidae\nActive\nCanis latrans\nCoyote, Eastern Coyote\nTrue\nFalse\nApproved\nPresent\nNaN\nNon-native\nNaN\nCommon\nNaN\nNaN\n8\n1\n0\nNaN\nSC\nNaN\nNaN\nG5\nME: S5\n\n\n3\nACAD\nAcadia National Park\nMammal\nCarnivora\nCanidae\nActive\nCanis lupus\nEastern Timber Wolf, Gray Wolf, Timber Wolf\nTrue\nFalse\nApproved\nUnconfirmed\nNaN\nNative\nNaN\nNaN\nNaN\nNaN\n2\n0\n0\nNaN\nE\nNaN\nNaN\nG5\nME: SH\n\n\n4\nACAD\nAcadia National Park\nMammal\nCarnivora\nCanidae\nActive\nVulpes vulpes\nBlack Fox, Cross Fox, Eastern Red Fox, Fox, Re...\nTrue\nFalse\nApproved\nPresent\nNaN\nUnknown\nNaN\nCommon\nBreeder\nNaN\n16\n0\n0\nNaN\nNaN\nNaN\nNaN\nG5\nME: S5\n\n\n\n\n\n\n\n\n# Encode categorical variables\nfor col in ['CategoryName', 'Order', 'Family', 'GRank', 'ParkName', 'Sensitive', 'Nativeness', 'Abundance','Observations', 'GRank']:\n    np_df[f\"{col}_cat\"] = np_df[col].astype('category').cat.codes\n\n# Split data into X and y\nX = np_df[['CategoryName_cat', 'Order_cat', 'Family_cat', 'GRank_cat', 'Sensitive_cat', 'Nativeness_cat', 'Abundance_cat','Observations_cat', 'GRank_cat']]\ny= np_df['ParkName_cat']\n\n# Split data into training and testing\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.33, random_state = 42)\n\n# Standardize the predictors\nscaler = StandardScaler() \nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\n\n2. Create a KNN Classifier\nAfter running an untuned model, iterate over different values of K to see which performs best. Then, visualize how your accuracy changes wtih a varying K.\n\n# Initialize KNN classiier\nknn = KNeighborsClassifier()\nknn.fit(X_train_scaled, y_train)\ny_pred = knn.predict(X_test_scaled)\n\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(accuracy)\n\n0.6029747149231532\n\n\n\n### Tune a KNN Model and Visualize results\ndef knn():\n    # Different k values to iterate over\n    k_values = [3, 5, 7, 9, 11]\n    accuracies = []\n\n    # Create KNN model for each value of K, fit/predict, and calculate accuracies\n    for k in k_values:\n        knn = KNeighborsClassifier(n_neighbors=k)\n        knn.fit(X_train_scaled, y_train)\n        \n        # Make predictions and calculate accuracy\n        y_pred = knn.predict(X_test_scaled)\n        accuracy = accuracy_score(y_test, y_pred)\n        accuracies.append(accuracy)\n        \n        print(f\"K={k} - Accuracy: {accuracy:.3f}\")\n    \n    # Visualize K against accuracy\n    plt.figure(figsize=(10,6))\n    plt.plot(k_values, accuracies, marker='o')\n    plt.xlabel('Number of Neighbors (K)')\n    plt.ylabel('Accuracy')\n    plt.title('KNN: Effect of K on Model Accuracy')\n    plt.grid(True)\n    plt.show()\n\nknn()\n\nK=3 - Accuracy: 0.593\nK=5 - Accuracy: 0.603\nK=7 - Accuracy: 0.600\nK=9 - Accuracy: 0.600\nK=11 - Accuracy: 0.597\n\n\n\n\n\n\n\n3. Create a Decision Tree\nAfter running an untuned model, iterate over different max depths for your decision tree to determine which performs best. Then,create a decision tree visual using plot_tree. Lastly, find the most important features using .feature_importances_.\n\n# Initialize Decision Tree classiier\ndt = DecisionTreeClassifier()\ndt.fit(X_train_scaled, y_train)\ny_pred = dt.predict(X_test_scaled)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(accuracy)\n\n0.6355478433316807\n\n\n\n# Max depths to iterate over\nmax_depths = [2, 3, 4, 5]\naccuracies = []\n\n# Create decision tree model for different depths and report accuracies\nfor depth in max_depths:\n    dt = DecisionTreeClassifier(max_depth=depth, random_state=42)\n    dt.fit(X_train_scaled, y_train)\n    \n    # Make predictions and calculate accuracy\n    y_pred = dt.predict(X_test_scaled)\n    accuracy = accuracy_score(y_test, y_pred)\n    accuracies.append(accuracy)\n    \n    print(f\"Max Depth: {depth} - Accuracy: {accuracy:.3f}\")\n\nMax Depth: 2 - Accuracy: 0.466\nMax Depth: 3 - Accuracy: 0.497\nMax Depth: 4 - Accuracy: 0.532\nMax Depth: 5 - Accuracy: 0.547\n\n\n\nVisualize Models\n\n# Create and fit model with best depth\ndt_best = DecisionTreeClassifier(max_depth=5, random_state=42)\ndt_best.fit(X_train_scaled, y_train)\n\n# Create the mapping from numeric classes to descriptive names\nclass_mapping = dict(zip(dt_best.classes_, np_df.ParkName.unique()))\n\n# Convert class labels in dt.classes_ to strings using the mapping\nclass_names_str = [class_mapping[cls] for cls in dt_best.classes_]\n\n# Plot decision tree\nplt.figure(figsize=(12, 15), dpi=700)\nplot_tree(dt_best, feature_names=X.columns, class_names= class_names_str, \n          filled=True, rounded=True)\nplt.title(\"Decision Tree Visualization (max_depth = 5)\")\nplt.savefig('decision_tree.png') \nplt.show()\n\n\n\n\n\n\nFind important features\n\nfeature_importance = pd.DataFrame({\n    'feature': X.columns,\n    'importance': dt_best.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"\\nFeature Importance:\")\nprint(feature_importance)\n\n\nFeature Importance:\n            feature  importance\n5    Nativeness_cat    0.705087\n0  CategoryName_cat    0.126157\n7  Observations_cat    0.092148\n6     Abundance_cat    0.057482\n1         Order_cat    0.009900\n3         GRank_cat    0.008710\n8         GRank_cat    0.000359\n2        Family_cat    0.000159\n4     Sensitive_cat    0.000000"
  },
  {
    "objectID": "discussion/week7.html",
    "href": "discussion/week7.html",
    "title": "Discussion 7",
    "section": "",
    "text": "Introduction\nIn this week’s discussion section, we will use a dataset containing tweets related to different disasters. For each observation (tweet), there is an outcome variable that classifies the disasters talked about in the tweet as real (1), or not (0). Rather than having multiple predictors as our X, we will have one predictor - the tweet. However, each individual word can be thought of as their own predictor, each contributing to predicting our outcome variable.\n\n\nData\nThe dataset this week is a commonly used dataset for NLP (Natural Language Processing). The dataset can be found here. Disasters.csv includes a text variable, which contains the tweet as a string. Our target variable, target, is a binary outcome variable with 1 representing the disaster discussed as real, and 0 representing the disaster discussed as not real.\n\n\nExcercise\n\nLoad in libraries and data\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nimport re\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Read in the data\ndisaster = pd.read_csv('../data/disaster.csv')\n\n\n\nClean text data\nWork with a partner and annotate what each line in the code chunk below is doing.\n\n# Cleaning of text data\ndef preprocess(text):\n    text = text.lower() # Converts to lowercase\n    text=text.strip()  # Removes leading/ trailing whitespace\n    text=re.sub(r'&lt;.*?&gt;','', text) # Remove html sytax \n    text = re.sub(r'[^\\w\\s]','',text)  # removes punctuation\n    text = re.sub(r'\\[[0-9]*\\]',' ',text)  # removes references\n    text = re.sub(r'\\d',' ',text) # removes digits\n    text = re.sub(r'\\s+', ' ', text) # collapse multiple spaces into a single space\n    return text\n\n\n# Apply string cleaning to text variable\ndisaster['clean_text'] = disaster['text'].apply(preprocess)\ndisaster.head()\n\n\n\n\n\n\n\n\nid\nkeyword\nlocation\ntext\ntarget\nclean_text\n\n\n\n\n0\n1\nNaN\nNaN\nOur Deeds are the Reason of this #earthquake M...\n1\nour deeds are the reason of this earthquake ma...\n\n\n1\n4\nNaN\nNaN\nForest fire near La Ronge Sask. Canada\n1\nforest fire near la ronge sask canada\n\n\n2\n5\nNaN\nNaN\nAll residents asked to 'shelter in place' are ...\n1\nall residents asked to shelter in place are be...\n\n\n3\n6\nNaN\nNaN\n13,000 people receive #wildfires evacuation or...\n1\npeople receive wildfires evacuation orders in...\n\n\n4\n7\nNaN\nNaN\nJust got sent this photo from Ruby #Alaska as ...\n1\njust got sent this photo from ruby alaska as s...\n\n\n\n\n\n\n\n\n\nWhat about stop words?\n\n# Proof that Tfidf veoctorizer excludes stopwords\nstop_words_ex = [\"On March 5th, I will crush my capstone presentation with my awesome team!\"]\n\nvectorizer_english = TfidfVectorizer(stop_words = \"english\")\nvectorizer_english.fit_transform(stop_words_ex)\n\nprint(\"Remaining words\")\nprint(vectorizer_english.get_feature_names_out())\n\nRemaining words\n['5th' 'awesome' 'capstone' 'crush' 'march' 'presentation' 'team']\n\n\n\n\nLogistic Regression\n\n# Split into test and train\nX_train, X_test, y_train, y_test = train_test_split(disaster[\"clean_text\"], disaster['target'], test_size = 0.3, random_state = 42)\n\n\n# Vectorize words \ntfidf_vectorizer = TfidfVectorizer(stop_words = \"english\")\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n\n\n# Initialize a logistic regression model and fit to vectorized training data\nlr_model = LogisticRegression(random_state = 42)\nlr_model.fit(X_train_tfidf, y_train)\ny_pred = lr_model.predict(X_test_tfidf)\n\n\n\nLogistic Regression Results\n\n# Calculate LR accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy}\")\n\n# Create confusion matrix for correctly/incorrectly predicting outcome variable\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize = (8,6))\nsns.heatmap(cm, annot = True, fmt = 'd', cmap = \"GnBu\",\n           xticklabels = [\"No disaster\", \"Disaster\"],\n           yticklabels = [\"No Disaster\", 'Disaster'])\nplt.title('Logisitc Regression Model Performance')\nplt.ylabel('True Label')\nplt.xlabel('Predicated Label')\nplt.show()\n\nAccuracy: 0.803415061295972\n\n\n\n\n\n\n\nTest model with new data\n\nnew_text = [\n    \"BREAKING: Massve earthquake hits the coast\",\n    \"I love watching disaster movies on Netflix\",\n    \"Thousands evacuated as hurricance approaches\",\n    \"Theeeesssss is a disassterrrrr\",\n    \"It's Windy!\",\n    \"The Palisade fire has damaged over 7,000 structures.\",\n    \"The Palisade wildfire has damaged over 7,000 structures.\",\n \n]\n\n# Preprocess new phrases\ncleaned_new_text =[preprocess(text) for text in new_text]\n\n# Transform using TF-IDF vectorizer\nnew_features = tfidf_vectorizer.transform(cleaned_new_text)\n\n# Make predictions\n\npredictions = lr_model.predict(new_features)\n\n# Check outcomes\nfor text, pred in zip(new_text, predictions):\n    print(f\"Text: {text}\")\n    print(f\"Prediction: {'Real Disaster' if pred == 1 else 'Not a Real Disaster'}\\n\")\n\nText: BREAKING: Massve earthquake hits the coast\nPrediction: Real Disaster\n\nText: I love watching disaster movies on Netflix\nPrediction: Not a Real Disaster\n\nText: Thousands evacuated as hurricance approaches\nPrediction: Real Disaster\n\nText: Theeeesssss is a disassterrrrr\nPrediction: Not a Real Disaster\n\nText: It's Windy!\nPrediction: Not a Real Disaster\n\nText: The Palisade fire has damaged over 7,000 structures.\nPrediction: Not a Real Disaster\n\nText: The Palisade wildfire has damaged over 7,000 structures.\nPrediction: Real Disaster"
  },
  {
    "objectID": "discussion/week2.html",
    "href": "discussion/week2.html",
    "title": "Discussion 2",
    "section": "",
    "text": "In this week’s discussion section, we will create some interactive plots to better undertsand how lasso and ridge regression are at work. To do so, we will use synthesized data that is made with the intention of better understanding how ridge and lasso regression are different based on the relationship of your parameters. It is important to note that your results with real data may look very different - unlike this notebook, the real world data you will be working with was not made to better understand regression models."
  },
  {
    "objectID": "discussion/week2.html#introduction",
    "href": "discussion/week2.html#introduction",
    "title": "Discussion 2",
    "section": "",
    "text": "In this week’s discussion section, we will create some interactive plots to better undertsand how lasso and ridge regression are at work. To do so, we will use synthesized data that is made with the intention of better understanding how ridge and lasso regression are different based on the relationship of your parameters. It is important to note that your results with real data may look very different - unlike this notebook, the real world data you will be working with was not made to better understand regression models."
  },
  {
    "objectID": "discussion/week2.html#data-loading",
    "href": "discussion/week2.html#data-loading",
    "title": "Discussion 2",
    "section": "Data Loading",
    "text": "Data Loading\nCopy the code below to load the neessary libraries genereate the data we will use. Read the comments to on each feature to get an idea of the relationship between variables.\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Lasso, Ridge\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom ipywidgets import interact, FloatLogSlider\n\n# Generate data\nnp.random.seed(42)\nn_samples = 200\nX = np.zeros((n_samples, 6))\nX[:, 0] = np.random.normal(0, 1, n_samples)  # X1 - Important feature\nX[:, 1] = np.random.normal(0, 1, n_samples)  # X2 -  Important feature\nX[:, 2] = X[:, 0] + np.random.normal(0, 0.1, n_samples)  # Correlated with X1\nX[:, 3] = X[:, 1] + np.random.normal(0, 0.1, n_samples)  # Correlated with X2\nX[:, 4] = np.random.normal(0, 0.1, n_samples)  # Noise\nX[:, 5] = np.random.normal(0, 0.1, n_samples)  # Noise\n\ny = 3 * X[:, 0] + 2 * X[:, 1] + 0.5 * X[:, 2] + np.random.normal(0, 0.1, n_samples)"
  },
  {
    "objectID": "discussion/week2.html#regression",
    "href": "discussion/week2.html#regression",
    "title": "Discussion 2",
    "section": "Regression",
    "text": "Regression\nNow that you have your data, do the following:\n\nSplit your data into training and testing.\n\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\nCreate and fit a ridge regression\n\n\n# Create and fit Ridge regression model\nridge_model = Ridge()\nridge_model.fit(X_train, y_train)\nridge_predictions = ridge_model.predict(X_test)\n\n\nCalculate the MSE and \\(R^2\\) for your ridge regression.\n\n\n# Calculate MSE and R^2 for Ridge regression\nridge_rmse = np.sqrt(mean_squared_error(y_test, ridge_predictions))\nridge_r2 = r2_score(y_test, ridge_predictions)\nprint(\"Ridge Regression RMSE:\", ridge_rmse)\nprint(\"Ridge Regression R²:\", ridge_r2)\n\nRidge Regression RMSE: 0.14410020171824725\nRidge Regression R²: 0.9984722762470866\n\n\n\nCreate and fit a lasso model.\n\n\n# Create and fit Lasso regression model\nlasso_model = Lasso() \nlasso_model.fit(X_train, y_train)\nlasso_predictions = lasso_model.predict(X_test)\n\n\nCalculate the MSE and \\(R^2\\) for your lasso model.\n\n\n# Calculate RMSE and R^2 for Lasso regression\nlasso_rmse = np.sqrt(mean_squared_error(y_test, lasso_predictions))\nlasso_r2 = r2_score(y_test, lasso_predictions)\nprint(\"Lasso Regression RMSE:\", lasso_rmse)\nprint(\"Lasso Regression R²:\", lasso_r2)\n\nLasso Regression RMSE: 1.2984978990079017\nLasso Regression R²: 0.8759496036905758"
  },
  {
    "objectID": "discussion/week2.html#visualizing-ridge-vs-regression",
    "href": "discussion/week2.html#visualizing-ridge-vs-regression",
    "title": "Discussion 2",
    "section": "Visualizing Ridge vs Regression",
    "text": "Visualizing Ridge vs Regression\n\nCreate a plot that looks at the alpha against the MSE for both lasso and ridge regression.\n\n\n# Visualize alphas against RMSE for lasso and ridge\n\n# Initialize lists to append data into\nrmse_lasso = []\nrmse_ridge = []\n\n# Define alpha values to iterate over\nalphas = [0.1,1,10]\n\n# Create and fit a lasso and ridge model for each predefined alpha\nfor alpha in alphas:\n    lasso = Lasso(alpha=alpha)\n    ridge = Ridge(alpha=alpha)\n    \n    lasso.fit(X_train, y_train)\n    ridge.fit(X_train, y_train)\n\n    # Calculate rmse for both models\n    rmse_lasso.append(np.sqrt(mean_squared_error(y_test, lasso.predict(X_test))))\n    rmse_ridge.append(np.sqrt(mean_squared_error(y_test, ridge.predict(X_test))))\n\n# Create plot of MSE again alpha values \nplt.figure(figsize=(10, 5))\nplt.plot(alphas, rmse_lasso, label='Lasso MSE')\nplt.plot(alphas, rmse_ridge, label='Ridge MSE')\nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('Root Mean Squared Error')\nplt.title('RMSE vs. Alpha for Lasso and Ridge Regression')\nplt.legend()\nplt.show()\n\n\n\n\n\nCreate an interactive plot (for both lasso and ridge) that allows you to adjust alpha to see how the actual vs predicted values are changing.\n\n\n# Create function to run model and create plot\n\ndef update_alphas(alpha, model_type):\n\n    # Condition to allow user to select different models\n    if model_type == 'Lasso':\n        model = Lasso(alpha=alpha)\n    else:\n        model = Ridge(alpha=alpha)\n\n    # Fit and predict model\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    # Calculate model metrics\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    r2 = r2_score(y_test, y_pred)\n\n    # Create plot of predicted values against actual values with line of best fit\n    plt.figure(figsize=(10, 5))\n    # Add predicted and actual values\n    plt.scatter(y_test, y_pred, color='blue', alpha=0.5, label=f'Predictions (alpha={alpha})')\n    # Add line of best fit\n    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n    plt.title(f'{model_type} Regression: Predictions vs Actual (alpha={alpha})')\n    plt.xlabel('Actual Values')\n    plt.ylabel('Predicted Values')\n    plt.legend()\n    # Adjust the position and aesthetics of the metric box\n    plt.figtext(0.5, -0.05, f'MSE: {rmse:.2f}, R²: {r2:.2f}', ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n    plt.show()\n\n# Create interactive widgets\n\n# Create alpha slider for choosing alpha value\nalpha_slider = FloatLogSlider(value= 0 , base=10, min=-3, max=3, step=0.1, description='Pick an Alpha!')\n\n# Create model selector for picking which model user wants to look at\nmodel_selector = {'Lasso': 'Lasso', 'Ridge': 'Ridge'}\n\n# Combine two widgets with model/plot output\ninteract(update_alphas, alpha=alpha_slider, model_type=model_selector)\n\n\n\n\n&lt;function __main__.update_alphas(alpha, model_type)&gt;\n\n\n\nCreate three different bar plots with the following guidelines: Each plot should represent a different alpha value: Alpha = 0.1, Alpha = 1, Alpha = 10 Each plot should show how both the ridge and lasso model performed The y axis should represent the six different variables: X1, X2, X1_corr, X2_corr, Noise1, Noise2. The y axis should represent the coefficients\n\n\n# Define alpha values to iterate over\nalphas = [0.1, 1.0, 10.0]\ndata = []\n\n# Create and fit ridge and lasso models and store coefficients in a new dataframe\nfor alpha in alphas:\n    ridge = Ridge(alpha=alpha).fit(X_train, y_train)\n    lasso = Lasso(alpha=alpha).fit(X_train, y_train)\n    data.append(pd.DataFrame({\n        'Ridge': ridge.coef_, # coef has as many indexes as there are variables\n        'Lasso': lasso.coef_\n    }, index=['X1', 'X2', 'X1_corr', 'X2_corr', 'Noise1', 'Noise2'])) # create feature names in new dataframe\n\n\n# Create barplot to visualize how coefficients change across alpha values and models\nfig, axes = plt.subplots(1, 3, figsize=(12, 4), sharey=True) \nfor i, df in enumerate(data): \n    df.plot.bar(ax=axes[i], width= 0.8)\n    axes[i].set_title(f'Alpha = {alphas[i]}')\n    axes[i].set_xticklabels(df.index, rotation=45)\n    \nplt.show()"
  },
  {
    "objectID": "discussion/week9.html",
    "href": "discussion/week9.html",
    "title": "Discussion 9",
    "section": "",
    "text": "Introduction\nIn this week’s discussion section, we will use simulated datasets to create a widget that looks at the results of a Support Vector Machine. The simulated datasents represent different relationships within our data. Our widget will allow us to select different kernels, regularization parameters, and methods for calculating gamma, allowing us to see how these changes change the classification of our data. The image below serves as a reminder of a few of the different kernels we can use in a Support Vector Machine.\n\n\n\nData\nAs mentioned above, the data we will be working with this week is all simulated. Copy the code cell below to obtain the data.\n\n\nExcercise\n\nLoad in libraries\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nimport ipywidgets as widgets\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\n\n\n\nCreate simulated data\n\ndef generate_environmental_datasets():\n    \"\"\"\n    Generate various environmental-themed synthetic datasets, including new datasets\n    tailored to test different characteristics in SVM modelling.\n    \"\"\"\n    np.random.seed(42)  # Ensure reproducibility\n\n    # 1. Pollution Impact Dataset\n    pollution_data = pd.DataFrame({\n        'industrial_emissions': np.random.normal(50, 15, 300),\n        'water_quality': np.random.normal(60, 10, 300),\n        'environmental_impact': np.zeros(300, dtype=int)\n    })\n    pollution_data.loc[\n        (pollution_data['industrial_emissions'] &gt; 60) & \n        (pollution_data['water_quality'] &lt; 50), \n        'environmental_impact'\n    ] = 2  # High Impact\n    pollution_data.loc[\n        (pollution_data['industrial_emissions'] &gt;= 40) & \n        (pollution_data['industrial_emissions'] &lt;= 60) & \n        (pollution_data['water_quality'] &gt;= 50) & \n        (pollution_data['water_quality'] &lt;= 70), \n        'environmental_impact'\n    ] = 1  # Medium Impact\n    \n    # 2. Climate Zones Dataset\n    climate_data = pd.DataFrame({\n        'temperature': np.random.normal(20, 10, 300),\n        'precipitation': np.random.normal(50, 20, 300),\n        'climate_zone': np.zeros(300, dtype=int)\n    })\n    climate_data.loc[\n        (climate_data['temperature'] &gt; 25) & \n        (climate_data['precipitation'] &gt; 70), \n        'climate_zone'\n    ] = 0  # Tropical\n    climate_data.loc[\n        (climate_data['temperature'] &lt; 10) & \n        (climate_data['precipitation'] &lt; 30), \n        'climate_zone'\n    ] = 1  # Arid\n    climate_data.loc[\n        (climate_data['temperature'] &gt;= 10) & \n        (climate_data['temperature'] &lt;= 25) & \n        (climate_data['precipitation'] &gt;= 30) & \n        (climate_data['precipitation'] &lt;= 70), \n        'climate_zone'\n    ] = 2  # Temperate\n\n    # Linearly Separable Dataset\n    X1 = np.random.randn(100, 2) + [2, 2]\n    X2 = np.random.randn(100, 2) + [5, 5]\n    X_linear = np.vstack((X1, X2))\n    y_linear = np.array([0]*100 + [1]*100)\n    df_linear = pd.DataFrame(X_linear, columns=['Temperature', 'Humidity'])\n    df_linear['Pollution_Level'] = y_linear\n\n    # Non-linearly Separable Data (Circular Boundaries)\n    length = 200\n    radius = 2\n    angle = np.linspace(0, 2 * np.pi, length)\n    X1_circular = np.vstack((np.sin(angle) * radius, np.cos(angle) * radius)).T + np.random.randn(length, 2) * 0.1\n    X2_circular = np.random.randn(length, 2) * 0.5\n    X_circular = np.vstack((X1_circular, X2_circular))\n    y_circular = np.array([0] * length + [1] * length)\n    df_circular = pd.DataFrame(X_circular, columns=['CO2_Emission', 'Water_Usage'])\n    df_circular['Area_Type'] = y_circular\n\n    # XOR-like Data\n    X1_xor = np.random.randn(50, 2) + [2, 2]\n    X2_xor = np.random.randn(50, 2) + [2, 5]\n    X3_xor = np.random.randn(50, 2) + [5, 2]\n    X4_xor = np.random.randn(50, 2) + [5, 5]\n    X_xor = np.vstack((X1_xor, X2_xor, X3_xor, X4_xor))\n    y_xor = np.array([0]*100 + [1]*100)\n    df_xor = pd.DataFrame(X_xor, columns=['Species_Count', 'Toxicity_Level'])\n    df_xor['Habitat_Damage'] = y_xor\n\n    # Overlapping Data\n    X1_overlap = np.random.randn(100, 2) + [3, 3]\n    X2_overlap = np.random.randn(100, 2) + [4, 4]\n    X_overlap = np.vstack((X1_overlap, X2_overlap))\n    y_overlap = np.array([0]*100 + [1]*100)\n    df_overlap = pd.DataFrame(X_overlap, columns=['Air_Quality', 'Noise_Level'])\n    df_overlap['Health_Risk'] = y_overlap\n\n    return {\n        'pollution_data': pollution_data,\n        'climate_data': climate_data,\n        'linear_data': df_linear,\n        'circular_data': df_circular,\n        'xor_data': df_xor,\n        'overlapping_data': df_overlap\n    }\n\n# Get the datasets\ndatasets = generate_environmental_datasets()\n\n\n\nCreate a SVM Widget"
  },
  {
    "objectID": "discussion/week8.html",
    "href": "discussion/week8.html",
    "title": "Discussion 8",
    "section": "",
    "text": "Introduction\nIn this week’s discussion section, we will use a dataset containing images of different plant diseases, and classify these images into different clusters. We will create a widget to see how our model classified a few of the images, as well as see how our classification changes when we change the value of K.\n\n\nData\nThe dataset this week is zipped file contain many different folders containg images of plants. Each folder represents a different plant disease, and all images in that folder house pictures representing the corresponding disease. The dataset can be found here.\n\n\nExcercise\n\nLoad in libraries and data\n\nimport numpy as np\nimport os\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom matplotlib.patches import Patch\nfrom ipywidgets import IntSlider, interact, Layout\nfrom IPython.display import display\nimport zipfile\n\n\n\nFunction to unzip the zipped plant data\n\ndef unzip(zip_path, extract_to):\n    # Ensure the extraction directory exists\n    if not os.path.exists(extract_to):\n        os.makedirs(extract_to)\n\n    # Open the zip file\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        # Extract all the contents into the directory\n        zip_ref.extractall(extract_to)\n        print(f\"Files extracted to {extract_to}\")\nunzip(\"../data/plant_disease.zip\", \"../data/plant_disease\")\n\nFiles extracted to ../data/plant_disease\n\n\nUse the function above to unzip your data folder. The first argument in the function is locating your zip file, and the second is picking a location/ file name for your new folder.\nunzip('/path/to/zipped/file.zip', 'path/to/unzipped/folder')\n\n\nNow that we have our data in the correct format (unzipped!), let’s preprocess our data.\n\n\n\nPreprocess data\n\nFunction to load image data\n\n# Function to open and standardize images used in model\n\ndef load_images(base_path, max_per_folder=20):\n    images = [] # Empty list to store images\n    labels = [] # Empty list to store label of each images\n    class_names = [] # Empty list to store the names of the folders for all images\n\n    for i, folder in enumerate(sorted(os.listdir(base_path))):\n        folder_path = os.path.join(base_path, folder) # Join base path with folders to iterate over\n        if not os.path.isdir(folder_path):\n            continue\n\n        class_names.append(folder)\n        print(f\"Loading from {folder}...\")\n\n        count = 0\n        for img_file in os.listdir(folder_path): # Iterate over each item in each folder\n            if count &gt;= max_per_folder: # Stop when counter gets to 20 images\n                break\n\n            if img_file.lower().endswith(('.png', '.jpg', '.jpeg')): # Ensure file in folder is correct format\n                try:\n                    img_path = os.path.join(folder_path, img_file)\n                    with Image.open(img_path) as img: # Open image\n                        img = img.convert('RGB') # Convert it to RGB to standardize color channels\n                        img = img.resize((100, 100), Image.Resampling.LANCZOS) # Resize image using LANCZOS resampling method\n\n                    images.append(np.array(img)) # Convert image to array and add to image list\n                    labels.append(i) # Add label to label list \n                    count += 1\n                except Exception as e: # Print error message if error with a file\n                    print(f\"Error with {img_file}: {e}\")\n\n    return np.array(images), np.array(labels), class_names\n\ndata_path = \"../data/plant_disease\"\nimages, labels, class_names = load_images(data_path)\nprint(f\"Loaded {len(images)} images from {len(class_names)} disease classes\")\n\nLoading from Apple___Apple_scab...\nLoading from Apple___Black_rot...\nLoading from Apple___Cedar_apple_rust...\nLoading from Apple___healthy...\nLoading from Background_without_leaves...\nLoading from Blueberry___healthy...\nLoading from Cherry___Powdery_mildew...\nLoading from Cherry___healthy...\nLoading from Corn___Cercospora_leaf_spot Gray_leaf_spot...\nLoading from Corn___Common_rust...\nLoading from Corn___Northern_Leaf_Blight...\nLoading from Corn___healthy...\nLoading from Grape___Black_rot...\nLoading from Grape___Esca_(Black_Measles)...\nLoading from Grape___Leaf_blight_(Isariopsis_Leaf_Spot)...\nLoading from Grape___healthy...\nLoading from Orange___Haunglongbing_(Citrus_greening)...\nLoading from Peach___Bacterial_spot...\nLoading from Peach___healthy...\nLoading from Pepper,_bell___Bacterial_spot...\nLoading from Pepper,_bell___healthy...\nLoading from Potato___Early_blight...\nLoading from Potato___Late_blight...\nLoading from Potato___healthy...\nLoading from Raspberry___healthy...\nLoading from Soybean___healthy...\nLoading from Squash___Powdery_mildew...\nLoading from Strawberry___Leaf_scorch...\nLoading from Strawberry___healthy...\nLoading from Tomato___Bacterial_spot...\nLoading from Tomato___Early_blight...\nLoading from Tomato___Late_blight...\nLoading from Tomato___Leaf_Mold...\nLoading from Tomato___Septoria_leaf_spot...\nLoading from Tomato___Spider_mites Two-spotted_spider_mite...\nLoading from Tomato___Target_Spot...\nLoading from Tomato___Tomato_Yellow_Leaf_Curl_Virus...\nLoading from Tomato___Tomato_mosaic_virus...\nLoading from Tomato___healthy...\nLoaded 780 images from 39 disease classes\n\n\n\n\n\nMore preprocessing …\n\nExtract features from data and perform PCA\n\n\n\nCreate an interactive widget and visualize clustering\n\nFunction to run a KMeans model and create a visualization"
  },
  {
    "objectID": "discussion/week1.html",
    "href": "discussion/week1.html",
    "title": "Discussion 1",
    "section": "",
    "text": "In this week’s discussion section, we will be using the same dataset from our weekly lab - Water characteristics in the Hudson River after Hurricane Irene. However, rather than looking at a single predictor variable, we are going to add more! Can we improve our model if we add more variables?? Let’s find out."
  },
  {
    "objectID": "discussion/week1.html#introduction",
    "href": "discussion/week1.html#introduction",
    "title": "Discussion 1",
    "section": "",
    "text": "In this week’s discussion section, we will be using the same dataset from our weekly lab - Water characteristics in the Hudson River after Hurricane Irene. However, rather than looking at a single predictor variable, we are going to add more! Can we improve our model if we add more variables?? Let’s find out."
  },
  {
    "objectID": "discussion/week1.html#data-loading",
    "href": "discussion/week1.html#data-loading",
    "title": "Discussion 1",
    "section": "Data Loading",
    "text": "Data Loading\nAccess the same .xlsx file we used in lab this week. If you lost access to it, you can find the data here. Instead of looking at only the dissolved oxygen and turbidity data this time, we are also going to read in data on rainfall. Read in each of these sheets on the excel sheet as its own dataframe. Load the following libraries:\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact\nimport ipywidgets as widgets\nfrom ipywidgets import interact, FloatSlider\nfrom IPython.display import display, clear_output\n\n\n# Load the data\nfp = '../data/Hurricane_Irene_Hudson_River.xlsx'\ndo_data = pd.read_excel(fp, sheet_name = 5).drop(['Piermont D.O. (ppm)'], axis = 1)\nrainfall_data = pd.read_excel(fp, sheet_name='Rainfall').drop(['Piermont  Rainfall Daily Accumulation (Inches)'], axis = 1)\nturbidity_data = pd.read_excel(fp, sheet_name='Turbidity').drop(['Piermont Turbidity in NTU'], axis = 1)"
  },
  {
    "objectID": "discussion/week1.html#data-wrangling",
    "href": "discussion/week1.html#data-wrangling",
    "title": "Discussion 1",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nPerform the following data wrangling steps to get our data ready for our model.\n\nMerge the three dataframes together. While merging, or after, drop all columns for the Piedmont location.\nUpdate the column names to be shorter and not have spaces. Use snake case.\nMake your date column a datetime obect.\nSet the data as the index for the merged dataframe.\n\n\n# Merge the two datasets on date\ndata = rainfall_data.merge(turbidity_data, on = 'Date Time (ET)')\ndata = data.merge(do_data, on = 'Date Time (ET)')\ndata.head()\n\n# Update the column names \ndata.columns = ['date', 'albany_rainfall', 'norrie_rainfall', 'albany_turbidity', 'norrie_turbidity','albany_do', 'norrie_do']\n\n# Convert data to datetime format and set it as index\ndata['date'] = pd.to_datetime(data['date'])\n\n# Update index\ndata.set_index('date', inplace=True)"
  },
  {
    "objectID": "discussion/week1.html#multiple-linear-regression",
    "href": "discussion/week1.html#multiple-linear-regression",
    "title": "Discussion 1",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nNow that our data is cleaned, let’s do the following to carry out a multiple linear regression.\n\nDefine your predictors and target variables.\nSplit the data into training and testing sets\nCreate and fit the model\nPredict and Evaluate your model\n\n\n# Define predictors and the target variable\nX = data[['albany_rainfall', 'norrie_rainfall', 'albany_do', 'norrie_do']]  # Adjust as needed\ny = data['albany_turbidity']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Create and fit the model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = model.predict(X_test)\nprint(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred))}\")\nprint(f\"R-squared: {r2_score(y_test, y_pred)}\")\n\nRMSE: 187.03290519070686\nR-squared: 0.6382523355891789"
  },
  {
    "objectID": "discussion/week1.html#create-a-widget-for-updating-the-predictor-and-target-variables.",
    "href": "discussion/week1.html#create-a-widget-for-updating-the-predictor-and-target-variables.",
    "title": "Discussion 1",
    "section": "Create a Widget for updating the predictor and target variables.",
    "text": "Create a Widget for updating the predictor and target variables.\n\nCreate the four different pieces to the widget: the predictor selector, the target selector, the evaluate button, and the output\nWrap our worfklow into a function called evaluate_model(). This function will run a linear regression model based on what the user selects as predictors and the outcome variable. It will print the \\(R^2\\), MSE, and a scatterplot of the actual versus predicted target variable.\nCreate a warning for your widget to ensure that the user does not select the same variable as both a predictor variable and a target variable.\nPlay around with your widget and see how your \\(R^2\\) changes based on your selected variables!\n\n\n# Create a widget for selecting predictors\npredictor_selector = widgets.SelectMultiple(\n    options=data.columns, # Options for predictor: columns of data\n    value=[data.columns[0]],  # Default selected: 1st column of data (albany_rainfall)\n    description='Predictors' # Name the predictor selection\n)\n\n# Create a dropdown for selecting the target variable\ntarget_selector = widgets.Dropdown(\n    options=data.columns, # Options for predictor: columns of data\n    value=data.columns[1],  # Default selected: 2nd column of data (norrie_rainfall)\n    description='Target',\n)\n\n# Create button to evaluate the model\nevaluate_button = widgets.Button(description=\"Evaluate Model\")\n\n# Output widget to display results\noutput = widgets.Output()\n\n# Define the function to handle button clicks\ndef evaluate_model(b):\n    with output:\n        clear_output(wait=True) # Clear previous displayed output before running\n        \n        # Make sure the target variable is not also a predictor variable\n        selected_predictors = [item for item in predictor_selector.value] # Pull out predictor values selected by user\n        if target_selector.value in selected_predictors: # Make sure target variable is not also a predictor variable\n            print(\"Target variable must not be in the predictors.\")\n            return\n        \n        # Assign X and y variables\n        X = data[selected_predictors]\n        y = data[target_selector.value]\n        \n        # Split data into training and testing sets\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n        \n        # Create and fit the model\n        model = LinearRegression()\n        model.fit(X_train, y_train)\n        \n        # Predict and calculate R^2 and MSE\n        y_pred = model.predict(X_test)\n        r2 = r2_score(y_test, y_pred)\n        mse = mean_squared_error(y_test, y_pred)\n        \n        # Display the R^2 score and MSE\n        print(f\"R^2: {r2:.4f}\")\n        print(f\"MSE: {mse:.4f}\")\n\n\n        # Create a scatter plot of y test vs predicted y\n        plt.scatter(y_test, y_pred) \n        plt.xlabel('Actual') \n        plt.ylabel('Predicted') \n        plt.title('Actual vs Predicted') \n        plt.show() \n\n\n# Display the widgets and connect the button to the function\ndisplay(predictor_selector, target_selector, evaluate_button, output)\nevaluate_button.on_click(evaluate_model)"
  },
  {
    "objectID": "discussion/week3.html",
    "href": "discussion/week3.html",
    "title": "Discussion 3",
    "section": "",
    "text": "In this week’s discussion section, we will create some plots to better undertsand how much class imbalances can effect our classification model. Rather than creating a widget that updates the parameters of the model ( like we have done in the past couple weeks), this week we will create a widget that updates our data - specifically updating the class imbalance within our data. To do so, we will use synthesized data that is made with the intention of better understanding how relationships within data for logistic regression work. It is important to note that your results with real data may look very different - unlike this notebook, the real world data you will be working with was not made to better understand logistic regression."
  },
  {
    "objectID": "discussion/week3.html#data",
    "href": "discussion/week3.html#data",
    "title": "Discussion 3",
    "section": "Data",
    "text": "Data\nWhile our data is synthetic, we will still have it hold an environmnetal value. Our data is going to represent the prescence/absence of the invasive European green crab that is often found in California coastal waters. These crabs prefer warmer water temperatures between 64° F and 79° F and salinity levels between 26 and 39 ppt. The features for our data will be water temperature and salinity, and our target variable will be the presence (1) or absence (0) of green crabs at our different sampling sites. Import the libraries below and copy the function to generate our data below to get started.\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact, FloatSlider\nimport seaborn as sns"
  },
  {
    "objectID": "discussion/week3.html#time-for-some-functions",
    "href": "discussion/week3.html#time-for-some-functions",
    "title": "Discussion 3",
    "section": "Time for some FUN(ctions)!",
    "text": "Time for some FUN(ctions)!\nWe will create six different functions for the different parts of our interactive output: one to generate the data, a second to create a barplot to represent the class imbalance, a third to create a confusion matrix, another to create an ROC curve, a function to wrap everything together, and a final function to add interactivity. Let’s get to it!\n\nFunction 1\nCreate a function that generates the species data. The parameters should be the sample size and the ratio of present green crabs.\n\ndef generate_species_data(n_samples=1000, presence_ratio=0.3):\n    # Calculate number of samples for each class\n    n_present = int(n_samples * presence_ratio)\n    n_absent = n_samples - n_present\n    \n    # Generate features for presence sites \n    # Green crabs prefer warmer temps (between 64 and 79 degrees Fahrenheit) and  salinity between 26 and 39 ppt\n    temp_present = np.random.normal(loc=71, scale= 4, size=n_present)\n    salinity_present = np.random.normal(loc=32, scale=3, size=n_present)\n    X_present = np.column_stack([temp_present, salinity_present])\n    y_present = np.ones(n_present)\n    \n    # Generate features for absence sites\n    # Sites with warmer temps or lower salinity\n    temp_absent = np.random.normal(loc=26, scale=3, size=n_absent)\n    salinity_absent = np.random.normal(loc=28, scale=2, size=n_absent)\n    X_absent = np.column_stack([temp_absent, salinity_absent])\n    y_absent = np.zeros(n_absent)\n    \n    # Combine and shuffle the data\n    X = np.concatenate([X_present, X_absent])\n    y = np.concatenate([y_present, y_absent])\n    \n    # Shuffle the data\n    shuffle_idx = np.random.permutation(n_samples)\n    X = X[shuffle_idx]\n    y = y[shuffle_idx]\n    \n    return X, y\n\n\n\nFunction 2\nCreate a function that creats a bar plot of species presense distribution based on the ratio selected by the user.\n\ndef plot_class_distribution(y):\n    plt.figure(figsize = (8,4))\n    \n    # Count the values in each category\n    class_counts = pd.Series(y).value_counts()\n    \n    # Create the barplot of Absent and Present species\n    sns.barplot(x = ['Absent', 'Present'], y = class_counts, color = '#005477')\n    plt.title('Distribution of Species Presence/Absence')\n    plt.ylabel('Number of Sampling sites')\n    \n    # Add percent over each bar\n    total = len(y)\n    for i,count in enumerate(class_counts):\n        percentage = count/total * 100 \n        plt.text(i, count, f'{percentage:.1f}%', ha = 'center', va = 'bottom')\n    plt.show()\n\n\n\nFunction 3\nCreate a function that plots a confusion matrix of the predicted y values and true y values.\n\ndef plot_confusion_matrix(y_true, y_pred):\n    \n    # Create confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Create confusion matrix plot\n    plt.figure(figsize = (8,6))\n    sns.heatmap(cm, fmt = 'd', cmap = 'GnBu',annot = True,\n               xticklabels = ['Absent', 'Present'],\n               yticklabels = ['Absent', 'Present'])\n    plt.title('Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.show()\n    \n    \n    # Calculate and display metrics\n    \n    TP = cm[1,1]\n    TN = cm[0,0]\n    FP = cm[0,1]\n    FN = cm[1,0]\n    \n    print(f\"True positives (correctly predicted presence): {TP}\")\n    \n    \n    # Calculate accuracy + various metric \n    accuracy = (TP + TN) / (TP + TN + FP + FN)\n    sensitivity = TP/ (TP + FN)\n    specificity = TN / (TN + FP )\n    \n    \n    print(f\"\\nModel Performance Metrics:\")\n    print(f\"Accuracy: {accuracy:.3f}\")\n    print(f\"Sensitivity ( True positive rate): {sensitivity:.3f}\")\n    print(f\"Specificity ( True negative rate:) {specificity:.3f}\")\n\n\n\nFunction 4\nCreate a function that plots an ROC curve using the predicted y class probabilities and true y values.\n\ndef plot_roc_curve(y_test, y_pred_prob):\n    \n    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n    \n    roc_auc = auc(fpr, tpr)\n    \n    plt.figure(figsize = (8,6))\n    plt.plot(fpr, tpr, color = 'darkorange', lw =2, label = f'ROC Curve (AUC = {roc_auc:.2f})')\n    plt.plot([0,1], [0,1], color = 'navy', lw = 2, linestyle = '--',\n            label = 'Random Classifier (AUC = 0.5)')\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC curve: Species Presence Prediction')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n\n\nFunction 5\nCreate function that runs a logistic regression and outputs the three plots you created above.\n\ndef interactive_logistic_regression(presence_ratio = 0.3):\n    \n    # Generate data based on class imbalance from user\n    X,y = generate_species_data(presence_ratio = presence_ratio)\n    \n    # Plot class distribution\n    print(\"\\nClass Distribution\")\n    plot_class_distribution(y)\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.3, random_state = 42)\n    \n    # Train model\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    \n    # Make predict\n    y_pred = model.predict(X_test)\n    y_pred_prob = model.predict_proba(X_test)[:,1]\n    \n    # Plot confusion matrix\n    print(\"\\nConfusion matrix:\")\n    plot_confusion_matrix(y_test, y_pred)\n    \n    # Plot ROC curve\n    print(\"\\nROC Curve:\")\n    plot_roc_curve(y_test, y_pred_prob)\n\n\n\nFunction 6\nCreate a function that adds interactivity to function 5.\n\n# Create interactive widget\n\ndef generate_log_regression():\n    interact(interactive_logistic_regression, \n            presence_ratio = FloatSlider(min = .1, max = .9, step= .1, value = 0.3,\n                                        description = \"% Present\"))\ngenerate_log_regression()"
  },
  {
    "objectID": "discussion/week6.html",
    "href": "discussion/week6.html",
    "title": "Discussion 6",
    "section": "",
    "text": "In this week’s discussion section, we will use a data with few NAs and intentionally add more NAs to it. We are going to run different imputation strategies on our newly “NA-ed” dataset, and see which performs best. Normally, you would never know how your imputation is actually performing, but this excercise will allow us to look under the hood a bit at how different imputation strategies perform differently. Once we find which imputation strategy works best, we will run a random forest on both the original data, as well as our newly imputed data. Which do you think will perform better??"
  },
  {
    "objectID": "discussion/week6.html#data",
    "href": "discussion/week6.html#data",
    "title": "Discussion 6",
    "section": "Data",
    "text": "Data\nThis week, we will be working with mushroom data! This dataset from the UCI Machine Learning Repository includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. Our target variable will be, poisonous, a categorical outcome variable classifying the mushroom as poisonous or not. We will include 22 features in our dataset that all relate to mushroom characteristics- such as cap-cut, cap-surface, bruises, and odor."
  },
  {
    "objectID": "discussion/week6.html#excercise",
    "href": "discussion/week6.html#excercise",
    "title": "Discussion 6",
    "section": "Excercise",
    "text": "Excercise\n\nImport Libraries\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.metrics import mean_squared_error, accuracy_score, r2_score\nfrom ucimlrepo import fetch_ucirepo \n\n\n\nLoad Data\n\n# Fetch dataset \nmushroom = fetch_ucirepo(id=73) \n  \n# Save data as X and y variables\nX = mushroom.data.features \ny = np.ravel(mushroom.data.targets)\n\n# Expand dataframe columns and look at view dataframe\npd.set_option('display.max_columns', None)\nX.head()\n\n\n\n\n\n\n\n\ncap-shape\ncap-surface\ncap-color\nbruises\nodor\ngill-attachment\ngill-spacing\ngill-size\ngill-color\nstalk-shape\nstalk-root\nstalk-surface-above-ring\nstalk-surface-below-ring\nstalk-color-above-ring\nstalk-color-below-ring\nveil-type\nveil-color\nring-number\nring-type\nspore-print-color\npopulation\nhabitat\n\n\n\n\n0\nx\ns\nn\nt\np\nf\nc\nn\nk\ne\ne\ns\ns\nw\nw\np\nw\no\np\nk\ns\nu\n\n\n1\nx\ns\ny\nt\na\nf\nc\nb\nk\ne\nc\ns\ns\nw\nw\np\nw\no\np\nn\nn\ng\n\n\n2\nb\ns\nw\nt\nl\nf\nc\nb\nn\ne\nc\ns\ns\nw\nw\np\nw\no\np\nn\nn\nm\n\n\n3\nx\ny\nw\nt\np\nf\nc\nn\nn\ne\ne\ns\ns\nw\nw\np\nw\no\np\nk\ns\nu\n\n\n4\nx\ns\ng\nf\nn\nf\nw\nb\nk\nt\ne\ns\ns\nw\nw\np\nw\no\ne\nn\na\ng\n\n\n\n\n\n\n\n\n\nEncoding Data\n\n# Factorize all columns\nfor col in X.columns: \n    X.loc[:, col] = pd.factorize(X[col], sort = True)[0]\n\n# View first few rows of encoded data\nX.iloc[0:5, 0:5]\n\n\n\n\n\n\n\n\ncap-shape\ncap-surface\ncap-color\nbruises\nodor\n\n\n\n\n0\n5\n2\n4\n1\n6\n\n\n1\n5\n2\n9\n1\n0\n\n\n2\n0\n2\n8\n1\n3\n\n\n3\n5\n3\n8\n1\n6\n\n\n4\n5\n2\n3\n0\n5\n\n\n\n\n\n\n\n\n\nTime to impute!\nDoes our dataset have any missing values? Lets check!\n\n# Check for NAs\nX.isna().sum()\n\ncap-shape                   0\ncap-surface                 0\ncap-color                   0\nbruises                     0\nodor                        0\ngill-attachment             0\ngill-spacing                0\ngill-size                   0\ngill-color                  0\nstalk-shape                 0\nstalk-root                  0\nstalk-surface-above-ring    0\nstalk-surface-below-ring    0\nstalk-color-above-ring      0\nstalk-color-below-ring      0\nveil-type                   0\nveil-color                  0\nring-number                 0\nring-type                   0\nspore-print-color           0\npopulation                  0\nhabitat                     0\ndtype: int64\n\n\nWe are going to randomly assign observations in our dataset to be missing, and then see which imputation methods perform best by comparing their results to our actual dataset. Let’s randomly assign NA observations throughout our dataset. We will create a copy of our dataframe and call it X_Na.\n\n# Create copy of X variables\nX_Na = X.copy()\n\n\n# Assign 10% of new dataframe with NA values\nfor col in X_Na.columns: \n    X_Na.loc[X_Na.sample(frac = 0.1).index, col] = np.nan\n\n\n# Check to make sure there are missing values\nX_Na.isna().sum()\n\ncap-shape                   812\ncap-surface                 812\ncap-color                   812\nbruises                     812\nodor                        812\ngill-attachment             812\ngill-spacing                812\ngill-size                   812\ngill-color                  812\nstalk-shape                 812\nstalk-root                  812\nstalk-surface-above-ring    812\nstalk-surface-below-ring    812\nstalk-color-above-ring      812\nstalk-color-below-ring      812\nveil-type                   812\nveil-color                  812\nring-number                 812\nring-type                   812\nspore-print-color           812\npopulation                  812\nhabitat                     812\ndtype: int64\n\n\nNow that we have our dataset with missing values, let’s impute!\n\nImputation method #1: Filling NA values with the mode\n\n# Impute with mode\nX_mode_impute = X_Na.fillna(X_Na.mode().iloc[0])\n\n# Check to make sure there are no NAs\nX_mode_impute.isna().sum()\n\ncap-shape                   0\ncap-surface                 0\ncap-color                   0\nbruises                     0\nodor                        0\ngill-attachment             0\ngill-spacing                0\ngill-size                   0\ngill-color                  0\nstalk-shape                 0\nstalk-root                  0\nstalk-surface-above-ring    0\nstalk-surface-below-ring    0\nstalk-color-above-ring      0\nstalk-color-below-ring      0\nveil-type                   0\nveil-color                  0\nring-number                 0\nring-type                   0\nspore-print-color           0\npopulation                  0\nhabitat                     0\ndtype: int64\n\n\n\n\nImputation method #2: Filling NA values with the median using SimpleImputer\n\n# Impute with median (using SimpleImputer) \nmedian_impute = SimpleImputer(strategy = 'median')\nX_median_impute = median_impute.fit_transform(X_Na)\nX_median_impute = pd.DataFrame(X_median_impute, columns = X.columns)\n\n# Check to make sure there are no NAs\nX_median_impute.isna().sum()\n\ncap-shape                   0\ncap-surface                 0\ncap-color                   0\nbruises                     0\nodor                        0\ngill-attachment             0\ngill-spacing                0\ngill-size                   0\ngill-color                  0\nstalk-shape                 0\nstalk-root                  0\nstalk-surface-above-ring    0\nstalk-surface-below-ring    0\nstalk-color-above-ring      0\nstalk-color-below-ring      0\nveil-type                   0\nveil-color                  0\nring-number                 0\nring-type                   0\nspore-print-color           0\npopulation                  0\nhabitat                     0\ndtype: int64\n\n\n\n\nImputation method #3: Filling NA values with KNN Imputer\n\n# Impute with KNN Imputer\nknn_impute = KNNImputer(n_neighbors = 20)\nX_knn_impute = knn_impute.fit_transform(X_Na)\nX_knn_impute = pd.DataFrame(X_knn_impute, columns = X_Na.columns)\n\n# Check to make sure there are no NAs\nX_knn_impute.isna().sum()\n\ncap-shape                   0\ncap-surface                 0\ncap-color                   0\nbruises                     0\nodor                        0\ngill-attachment             0\ngill-spacing                0\ngill-size                   0\ngill-color                  0\nstalk-shape                 0\nstalk-root                  0\nstalk-surface-above-ring    0\nstalk-surface-below-ring    0\nstalk-color-above-ring      0\nstalk-color-below-ring      0\nveil-type                   0\nveil-color                  0\nring-number                 0\nring-type                   0\nspore-print-color           0\npopulation                  0\nhabitat                     0\ndtype: int64\n\n\n\n\nNow that we have four different dataframes with four different imputation methods, lets see which best captured our real data!We can do this using the mean squared error!\n\n# Calculate imputation accuracy using mean squared error\nmse_mode = mean_squared_error(X, X_mode_impute)\nmse_median = mean_squared_error(X, X_median_impute)\nmse_knn = mean_squared_error(X, X_knn_impute)\n\n\n# Report results\nprint(f\"Mode imputation performance: {mse_mode}\")\nprint(f\"Median Imputation performance: {mse_median}\")\nprint(f\"KNN Imputation performance: {mse_knn}\")\n\nMode imputation performance: 0.4570296763797503\nMedian Imputation performance: 0.262935857839846\nKNN Imputation performance: 0.1019636402802023\n\n\n\n# Calculate imputation accuracy using R2\nr2_mode = r2_score(X, X_mode_impute)\nr2_median = r2_score(X, X_median_impute)\nr2_knn = r2_score(X, X_knn_impute)\n\n\n# Report results\nprint(f\"Mode imputation performance: {r2_mode}\")\nprint(f\"Median Imputation performance: {r2_median}\")\nprint(f\"KNN Imputation performance: {r2_knn}\")\n\nMode imputation performance: 0.8514394805461396\nMedian Imputation performance: 0.8837577773301519\nKNN Imputation performance: 0.9676924025826152\n\n\nIt looks like our KNN Imputation was the most successfull in imputing NAs! Let’s run a random forest with our actual data, and our KNN imputed data to see how/if they differ!\n\n\n\nRandom Forest Classifier with original data\n\n# Split actual data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n\n\n# Number of features to include for tuning\nnum_features = [1,4,7,10,13,16,19,22]\naccuracy = []\n\nfor feature in num_features: \n    rf_classifier = RandomForestClassifier(\n        n_estimators = 50, \n        max_depth = 3, \n        random_state = 42, \n        max_features = feature\n    )\n    \n    # Train model\n    rf_classifier.fit(X_train, y_train)\n    \n    # Predict and evaluate \n    y_pred = rf_classifier.predict(X_test)\n    rf_accuracy = accuracy_score(y_test, y_pred)\n    accuracy.append(rf_accuracy)\n    print(f\"Number of features: {feature}; Random Forest Accuracy: {rf_accuracy}\")\n\nNumber of features: 1; Random Forest Accuracy: 0.916735028712059\nNumber of features: 4; Random Forest Accuracy: 0.9848236259228876\nNumber of features: 7; Random Forest Accuracy: 0.9868744872846595\nNumber of features: 10; Random Forest Accuracy: 0.9835931091058244\nNumber of features: 13; Random Forest Accuracy: 0.9823625922887613\nNumber of features: 16; Random Forest Accuracy: 0.9860541427399507\nNumber of features: 19; Random Forest Accuracy: 0.9819524200164069\nNumber of features: 22; Random Forest Accuracy: 0.9577522559474979\n\n\n\n\nRandom Forest Classifier with imputed data:\n\n# Split imputed data \nX_train, X_test, y_train, y_test = train_test_split(X_knn_impute, y, test_size = 0.3, random_state = 42)\n\n\n# Number of features to include for tuning\n# Number of features to include for tuning\nnum_features = [1,4,7,10,13,16,19,22]\naccuracy = []\n\nfor feature in num_features: \n    rf_classifier = RandomForestClassifier(\n        n_estimators = 50, \n        max_depth = 3, \n        random_state = 42, \n        max_features = feature\n    )\n    \n    # Train model\n    rf_classifier.fit(X_train, y_train)\n    \n    # Predict and evaluate \n    y_pred = rf_classifier.predict(X_test)\n    rf_accuracy = accuracy_score(y_test, y_pred)\n    accuracy.append(rf_accuracy)\n    print(f\"Number of features: {feature}; Random Forest Accuracy: {rf_accuracy}\")\n\nNumber of features: 1; Random Forest Accuracy: 0.9183757178014766\nNumber of features: 4; Random Forest Accuracy: 0.977850697292863\nNumber of features: 7; Random Forest Accuracy: 0.9848236259228876\nNumber of features: 10; Random Forest Accuracy: 0.985233798195242\nNumber of features: 13; Random Forest Accuracy: 0.9827727645611156\nNumber of features: 16; Random Forest Accuracy: 0.9835931091058244\nNumber of features: 19; Random Forest Accuracy: 0.970467596390484\nNumber of features: 22; Random Forest Accuracy: 0.9561115668580804"
  }
]